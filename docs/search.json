[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brynjólfur Gauti Guðrúnar Jónsson",
    "section": "",
    "text": "I am a statistics PhD student at the University of Iceland researching spatially distributed extreme values. I also teach Statistical consulting at the University of Iceland."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Brynjólfur Gauti Guðrúnar Jónsson",
    "section": "",
    "text": "I am a statistics PhD student at the University of Iceland researching spatially distributed extreme values. I also teach Statistical consulting at the University of Iceland."
  },
  {
    "objectID": "about.html#talks-interviews-etc.",
    "href": "about.html#talks-interviews-etc.",
    "title": "Brynjólfur Gauti Guðrúnar Jónsson",
    "section": "Talks, interviews etc.",
    "text": "Talks, interviews etc.\n\n\n\n\n\n\n\n\n\n\nDatabeers #2\n\n\nI gave a talk at the 2nd Databeers event hosted by Lucinity. There I talked about open data and helping people make sense of official or press statements by putting them…\n\n\n\n\n\n\nSep 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterview on the University website\n\n\nHalldór Marteinsson interviewed me for the University of Iceland website about statistics, covid-19 predictions and more.\n\n\n\n\n\n\nDec 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStanCon2020: COVID-19 Prediction Model\n\n\nI gave a talk about the statistical methodology behind the prediction model used by the Icelandic government during the first wave of the COVID-19 pandemic.\n\n\n\n\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/databeers_2022_09_16/index.html",
    "href": "talks/databeers_2022_09_16/index.html",
    "title": "Databeers #2",
    "section": "",
    "text": "The presentations were in the PechaKucha format:\n\n20 slides\n20 seconds per slide\n\n\n\nLink to the slides in full size"
  },
  {
    "objectID": "phd/articles/results/spatial/index.html",
    "href": "phd/articles/results/spatial/index.html",
    "title": "The Smooth Step",
    "section": "",
    "text": "Code\nlibrary(bggjphd)\nlibrary(tidyverse)\nlibrary(bayesplot)\nlibrary(posterior)\nlibrary(GGally)\nlibrary(scales)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(arrow)\ntheme_set(theme_bggj())\nThe latent parameters, \\(\\psi\\), \\(\\tau\\), \\(\\phi\\), and \\(\\gamma\\), are given intrinsic random walk spatial priors, for example\n\\[\n\\begin{aligned}\n\\psi &\\sim \\mathcal N(\\mathbf 0, \\tau_\\psi \\cdot Q_u) \\\\\n\\sigma_\\psi &= \\frac{1}{\\sqrt\\tau_\\psi} \\\\\n\\sigma_\\psi &\\sim \\mathrm{Exp}(1)\n\\end{aligned}\n\\]\nHere, \\(Q_u\\) is defined by\n\\[\nQ_u = R \\otimes I + I \\otimes R,\n\\]\nwhere \\(I\\) is the identity matrix and\n\\[\nR = \\begin{bmatrix}\n1 & -1 & & & & & \\\\\n-1 & 2 & -1 & & & & \\\\\n& -1 & 2 & -1 & & & \\\\\n& & \\ddots & \\ddots & \\ddots & & \\\\\n& & &-1 &2 &-1 & \\\\\n& & & & -1 & 1\\\\\n\\end{bmatrix}.\n\\]\nThe results were obtained by running ms_smooth() in parrallel on four cores with four chains each run for 4000 samples. Half of those samples were designated as warm-up and so we have a total of 8000 samples from the posterior.\nCode\ntheta_results &lt;- read_parquet(\"data/theta_results.parquet\") |&gt; \n  as_draws_df()\n\nstation_results &lt;- read_parquet(\"data/station_results.parquet\")\n\nn_iter &lt;- theta_results |&gt; \n  as_tibble() |&gt; \n  pull(.iteration) |&gt; \n  max()"
  },
  {
    "objectID": "phd/articles/results/spatial/index.html#trace-plots",
    "href": "phd/articles/results/spatial/index.html#trace-plots",
    "title": "The Smooth Step",
    "section": "Trace plots",
    "text": "Trace plots\n\n\nCode\ntheta_results |&gt; \n  filter(.chain != 4) |&gt; \n  filter(.iteration &gt; 1) |&gt;\n  mcmc_trace()"
  },
  {
    "objectID": "phd/articles/results/spatial/index.html#autocorrelation-functions",
    "href": "phd/articles/results/spatial/index.html#autocorrelation-functions",
    "title": "The Smooth Step",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\n\nCode\ntheta_results |&gt; \n  filter(.iteration &gt; 1) |&gt; \n  mcmc_acf_bar()"
  },
  {
    "objectID": "phd/articles/results/spatial/index.html#acceptance-probability",
    "href": "phd/articles/results/spatial/index.html#acceptance-probability",
    "title": "The Smooth Step",
    "section": "Acceptance probability",
    "text": "Acceptance probability\n\n\nCode\ntheta_results |&gt; \n  subset_draws(\"theta[1]\") |&gt; \n  as_tibble() |&gt; \n  rename(value = \"theta[1]\") |&gt; \n  group_by(.chain) |&gt; \n  mutate(accept = 1 * (value != lag(value))) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(.iteration, accept, group = .chain)) +\n  geom_smooth(method = \"loess\", span = 0.3, se = 0) +\n  scale_x_continuous(\n    expand = expansion()\n  ) +\n  scale_y_continuous(\n    breaks = pretty_breaks(5),\n    labels = label_percent(),\n    expand = expansion()\n  ) +\n  theme(\n    plot.margin = margin(t = 5, r = 35, b = 5, l = 5)\n  ) +  \n  coord_cartesian(ylim = c(0, 1)) +\n  labs(\n    x = \"Iteration\",\n    y = \"Acceptance probability\",\n    title = \"Acceptance probability for theta[1]\"\n  )"
  },
  {
    "objectID": "phd/articles/results/spatial/index.html#hyperpriors",
    "href": "phd/articles/results/spatial/index.html#hyperpriors",
    "title": "The Smooth Step",
    "section": "Hyperpriors",
    "text": "Hyperpriors\n\nLog precision scale\n\n\nCode\ntheta_results |&gt; \n  filter(.iteration &gt; 1) |&gt; \n  summarise_draws() |&gt; \n  kable(digits = 3) |&gt; \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta[1]\n6.247\n6.247\n0.020\n0.022\n6.215\n6.282\n1.068\n41.341\n55.597\n\n\ntheta[2]\n5.345\n5.345\n0.019\n0.022\n5.315\n5.375\n1.056\n30.657\n53.149\n\n\ntheta[3]\n6.193\n6.192\n0.027\n0.027\n6.151\n6.243\n1.141\n16.002\n16.160\n\n\ntheta[4]\n15.850\n15.851\n0.045\n0.047\n15.775\n15.923\n1.172\n14.069\n65.501\n\n\n\n\n\n\n\n\n\nCode\ntheta_results |&gt; \n  filter(.iteration &gt; 1) |&gt; \n  mcmc_hist_by_chain(\n  )\n\n\n\n\n\n\n\n\n\n\n\nOn standard deviation scale\n\n\nCode\ntheta_results |&gt; \n  filter(.iteration &gt; 1) |&gt; \n  summarise_draws() |&gt; \n  kable(digits = 3) |&gt; \n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\ntheta[1]\n6.247\n6.247\n0.020\n0.022\n6.215\n6.282\n1.068\n41.341\n55.597\n\n\ntheta[2]\n5.345\n5.345\n0.019\n0.022\n5.315\n5.375\n1.056\n30.657\n53.149\n\n\ntheta[3]\n6.193\n6.192\n0.027\n0.027\n6.151\n6.243\n1.141\n16.002\n16.160\n\n\ntheta[4]\n15.850\n15.851\n0.045\n0.047\n15.775\n15.923\n1.172\n14.069\n65.501\n\n\n\n\n\n\n\n\n\nCode\ntheta_results |&gt; \n  filter(.iteration &gt; 1) |&gt; \n  mcmc_hist_by_chain(\n    transformations = function(x) exp(-x/2)\n  )"
  },
  {
    "objectID": "phd/articles/results/spatial/index.html#gev-parameters",
    "href": "phd/articles/results/spatial/index.html#gev-parameters",
    "title": "The Smooth Step",
    "section": "GEV Parameters",
    "text": "GEV Parameters\n\nComparing ML and MCMC estimates\n\n\nCode\nstation_results |&gt; \n  pivot_longer(c(ml_estimate, mcmc_mean)) |&gt; \n  mutate(\n    variable = fct_relevel(\n      factor(variable),\n      \"psi\", \"tau\", \"phi\", \"gamma\"\n    ),\n    name = fct_recode(\n      factor(name),\n      \"Maximum Likelihood\" = \"ml_estimate\",\n      \"Posterior Mean\" = \"mcmc_mean\"\n    )\n  ) |&gt; \n  ggplot(aes(value)) +\n  geom_histogram() +\n  facet_wrap(vars(variable, name), ncol = 2, scales = \"free_x\") +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distributions of station parameters from ML and MCMC\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nstation_results |&gt; \n  ggplot(aes(ml_estimate, mcmc_mean)) +\n  geom_abline(intercept = 0, slope = 1, lty = 2) +\n  geom_point(alpha = 0.1) +\n  facet_wrap(\"variable\", scales = \"free\") +\n  labs(\n    x = \"ML Estimate (Max step)\",\n    y = \"Posterior Mean (Smooth step)\",\n    title = \"Comparing estimates from the Max and the Smooth steps\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nstation_results |&gt; \n  pivot_longer(c(ml_estimate, mcmc_mean)) |&gt; \n  pivot_wider(names_from = variable) |&gt; \n  mutate(\n    mu = exp(psi),\n    sigma = exp(psi + tau),\n    xi = link_shape_inverse(phi),\n    delta = link_trend_inverse(gamma)\n  ) |&gt; \n  select(-(gamma:tau)) |&gt; \n  pivot_longer(c(mu:delta), names_to = \"variable\") |&gt; \n  pivot_wider() |&gt; \n  ggplot(aes(ml_estimate, mcmc_mean)) +\n  geom_abline(intercept = 0, slope = 1, lty = 2) +\n  geom_point(alpha = 0.1) +\n  facet_wrap(\"variable\", scales = \"free\") +\n  labs(\n    x = \"ML Estimate (Max step)\",\n    y = \"Posterior Mean (Smooth step)\",\n    title = \"Comparing estimates from the Max and the Smooth steps\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSpatial Distributions\n\n\nCode\nproj_plot &lt;- function(data) {\n  \n  title &lt;- str_c(\n    \"Spatial distribution of estimates for \", unique(data$variable)\n  )\n  \n  plot_dat &lt;- data |&gt; \n    pivot_longer(c(ml_estimate, mcmc_mean)) |&gt; \n    mutate(\n      name = fct_recode(\n        factor(name),\n        \"Maximum Likelihood\" = \"ml_estimate\",\n        \"Posterior Mean\" = \"mcmc_mean\"\n      )\n    ) |&gt; \n    group_by(name) |&gt; \n    mutate(\n      value = (value - mean(value)) / sd(value)\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      value = case_when(\n        name == \"Posterior Mean\" ~ value,\n        value &lt; quantile(value, 0.0025) ~ quantile(value, 0.0025),\n        value &gt; quantile(value, 0.9975) ~ quantile(value, 0.9975),\n        TRUE ~ value\n      )\n    )\n  \n  min_val &lt;- min(plot_dat$value)\n  max_val &lt;- max(plot_dat$value)\n  \n  lim_range &lt;- max(abs(min_val), abs(max_val))\n  \n  limits &lt;- c(-1, 1) * lim_range\n  \n  plot_dat |&gt; \n    ggplot(aes(proj_x, proj_y)) +\n    geom_raster(aes(fill = value)) +\n    scale_fill_viridis_c(limits = limits) +\n    facet_wrap(\"name\", nrow = 1) +\n    coord_cartesian(expand = FALSE) +\n    labs(\n      title = title,\n      fill = NULL,\n      x = \"X projection\",\n      y = \"Y projection\"\n    )\n}\n\n\n\nLocation\n\npsi\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"psi\") |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\nmu\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"psi\") |&gt; \n  mutate(variable = \"mu\") |&gt; \n  mutate_at(vars(ml_estimate, mcmc_mean), exp) |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\n\nScale\n\ntau\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"tau\") |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\nsigma\n\n\nCode\nstation_results |&gt; \n  filter(variable %in% c(\"tau\", \"psi\")) |&gt; \n  pivot_longer(c(ml_estimate, mcmc_mean)) |&gt; \n  pivot_wider(names_from = variable, values_from = value) |&gt; \n  mutate(sigma = exp(tau + psi)) |&gt; \n  select(-psi, -tau) |&gt; \n  pivot_longer(c(sigma), names_to = \"variable\", values_to = \"value\") |&gt; \n  pivot_wider() |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\n\nShape\n\nphi\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"phi\") |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\nxi\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"phi\") |&gt; \n  mutate(variable = \"xi\") |&gt; \n  mutate_at(\n    vars(mcmc_mean, ml_estimate),\n    link_shape_inverse\n  ) |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\n\nTrend\n\ngamma\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"gamma\") |&gt; \n  proj_plot()\n\n\n\n\n\n\n\n\n\n\n\nDelta\n\n\nCode\nstation_results |&gt; \n  filter(variable == \"gamma\") |&gt; \n  mutate(variable = \"delta\") |&gt; \n  mutate_at(\n    vars(mcmc_mean, ml_estimate),\n    link_trend_inverse\n    ) |&gt; \n  proj_plot()"
  },
  {
    "objectID": "phd/articles/results/max/index.html",
    "href": "phd/articles/results/max/index.html",
    "title": "The Max Step",
    "section": "",
    "text": "In our case we want \\(\\mu\\) to vary with time. If we write \\(y_{it}\\) for the observed hourly maximum at station \\(i\\) during year \\(t\\) we will then have\nwhere\nand"
  },
  {
    "objectID": "phd/articles/results/max/index.html#transformed-parameters",
    "href": "phd/articles/results/max/index.html#transformed-parameters",
    "title": "The Max Step",
    "section": "Transformed parameters",
    "text": "Transformed parameters\nHaving performed the Max step and saved the ML estimates we can easily load them by fetching the object station_estimates. Here we plot the distribution of transformed estimates.\n\\[\\begin{aligned}\n\\psi &= \\log(\\mu) \\\\\n\\tau &= \\log(\\frac{\\sigma}{\\mu}) = \\log(\\sigma) - \\log(\\mu) \\\\\n\\phi &= h(\\xi) \\\\\n\\gamma &= d(\\Delta),\n\\end{aligned}\\]\nwhere the link functions for the shape and trend parameters are defined according to Johannesson et al. (2021)\n\nJohannesson, Árni V., Stefan Siegert, Raphaël Huser, Haakon Bakka, and Birgir Hrafnkelsson. 2021. “Approximate Bayesian Inference for Analysis of Spatio-Temporal Flood Frequency Data,” April. https://doi.org/10.48550/arXiv.1907.04763.\n\\[\\begin{aligned}\nh(\\xi) &= a_\\phi + b_\\phi \\log\\left(-\\log\\left[1 - \\left(\\xi + \\frac12\\right)^{c_\\phi}\\right]\\right) \\\\\nc_\\phi &= 0.8 \\\\\nb_\\phi &= -\\frac{1}{c_\\phi}\\log\\left(1 - \\frac{1}{2^{c_\\phi}}\\right)\\left(1 - \\frac{1}{2^{c_\\phi}}\\right) 2^{c_\\phi - 1} \\\\\na_\\phi &= -b_\\phi \\log\\left(-\\log(1 - \\frac{1}{2^{c_\\phi}})\\right) \\\\\n\n\\newline\n\nd(\\Delta) &= \\frac12 \\delta_0 \\left(\\log(\\delta_0 + \\Delta) - \\log(\\delta_0 - \\Delta_i)\\right) \\\\\n\\delta_0 &= 0.008.\n\\end{aligned}\\]"
  },
  {
    "objectID": "phd/articles/results/max/index.html#distributions",
    "href": "phd/articles/results/max/index.html#distributions",
    "title": "The Max Step",
    "section": "Distributions",
    "text": "Distributions\n\nTransformed Scale\n\n\nCode\nd_prior &lt;- crossing(\n  name = c(\"phi\", \"gamma\"),\n  x = seq(-1, 1, len = 200)\n) |&gt; \n  mutate(\n    x = ifelse(name == \"phi\", x * 0.8, x * 0.01),\n    y = ifelse(name == \"phi\", prior_shape(x) |&gt; exp(), prior_trend(x) |&gt; exp()),\n    y = ifelse(name == \"phi\", y * 6000, y * 0.05)\n  )\n\n\nd |&gt; \n  ggplot(aes(value)) +\n  geom_histogram(bins = 60) +\n  geom_line(\n    data = d_prior,\n    aes(x = x, y = y * 1e3),\n    inherit.aes = F\n  ) +\n  facet_wrap(\"name\", scales = \"free\") +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distributions of GEV parameters from Max step\",\n    subtitle = \"The superimposed curves are the implied prior distributions\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; \n  pivot_wider() |&gt; \n  select(-station, -proj_x, -proj_y) |&gt; \n  ggpairs(progress = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nOriginal Scale\n\n\nCode\nd_prior &lt;- crossing(\n  name = c(\"phi\", \"gamma\"),\n  x = seq(-1, 1, len = 200)\n) |&gt; \n  mutate(\n    x = ifelse(name == \"phi\", x * 0.8, x * 0.01),\n    y = ifelse(name == \"phi\", prior_shape(x) |&gt; exp(), prior_trend(x) |&gt; exp()),\n    y = ifelse(name == \"phi\", y * 2000000, y * 20),\n    x = ifelse(name == \"phi\", link_shape_inverse(x), link_trend_inverse(x)),\n    name = ifelse(name == \"phi\", \"xi\", \"delta\")\n  )\n\nd |&gt; \n  pivot_wider() |&gt; \n  mutate(mu = exp(psi),\n         sigma = exp(tau + psi),\n         xi = link_shape_inverse(phi),\n         delta = link_trend_inverse(gamma)) |&gt; \n  select(-psi, -tau, -phi, -gamma, -proj_x, -proj_y) |&gt; \n  pivot_longer(c(-station)) |&gt; \n  mutate(name = fct_relevel(name, \"mu\", \"sigma\", \"xi\", \"delta\")) |&gt; \n  ggplot(aes(value)) +\n  geom_histogram(bins = 100) +\n  geom_line(\n    data = d_prior,\n    aes(x = x, y = y),\n    inherit.aes = F\n  ) +\n  facet_wrap(\"name\", scales = \"free\") +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distributions of backtransformed GEV parameters from Max step\",\n    subtitle = \"The superimposed curves are the implied prior distributions\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; \n  pivot_wider() |&gt; \n  mutate(mu = exp(psi),\n         sigma = exp(tau + psi),\n         xi = link_shape_inverse(phi),\n         delta = link_trend_inverse(gamma)) |&gt; \n  select(-psi, -tau, -phi, -gamma, -station, -proj_x, -proj_y) |&gt; \n  ggpairs(progress = FALSE)"
  },
  {
    "objectID": "phd/articles/results/max/index.html#spatial-distribution",
    "href": "phd/articles/results/max/index.html#spatial-distribution",
    "title": "The Max Step",
    "section": "Spatial Distribution",
    "text": "Spatial Distribution\n\nLocation\n\n\nCode\nd |&gt; \n  filter(name == \"psi\") |&gt; \n  ggplot(aes(proj_x, proj_y, fill = value)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Psi\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; \n  filter(name == \"psi\") |&gt; \n  ggplot(aes(proj_x, proj_y, fill = exp(value))) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Mu\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nScale\n\n\nCode\nd |&gt; \n  filter(name == \"tau\") |&gt; \n  ggplot(aes(proj_x, proj_y, fill = value)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Tau\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; \n  pivot_wider() |&gt; \n  mutate(sigma = exp(tau + psi)) |&gt; \n  ggplot(aes(proj_x, proj_y, fill = sigma)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Sigma\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nShape\n\n\nCode\nd |&gt; \n  filter(name == \"phi\") |&gt; \n  ggplot(aes(proj_x, proj_y, fill = value)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Phi\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; \n  pivot_wider() |&gt; \n  mutate(xi = link_shape_inverse(phi)) |&gt; \n  ggplot(aes(proj_x, proj_y, fill = xi)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Xi\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nTrend\n\n\nCode\nd |&gt; \n  filter(name == \"gamma\") |&gt; \n  ggplot(aes(proj_x, proj_y, fill = value)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Gamma\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nd |&gt; \n  pivot_wider() |&gt; \n  mutate(delta = link_trend_inverse(gamma)) |&gt; \n  ggplot(aes(proj_x, proj_y, fill = delta)) +\n  geom_raster(interpolate = TRUE) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_x), pretty(d$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(d$proj_y), pretty(d$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = NULL,\n    title = \"Spatial distribution of Delta\"\n  )"
  },
  {
    "objectID": "phd/articles/methods/code/max/index.html",
    "href": "phd/articles/methods/code/max/index.html",
    "title": "The Max Step",
    "section": "",
    "text": "Log likelihood\n\nneg_log_lik_gev_trend &lt;- function(\n    y,\n    t,\n    par,\n    priors,\n    links,\n    t0 = 1981\n) {\n  t &lt;- t - t0\n  \n  mu0 &lt;- exp(par[1])\n  sigma &lt;- exp(par[2] + par[1])\n  xi &lt;- link_shape_inverse(par[3])\n  delta &lt;- link_trend_inverse(par[4])\n  \n  mu &lt;- mu0 * (1 + delta * t)\n  \n  z &lt;- (y - mu) / sigma\n  \n  if (any(1 + xi * z &lt;= 0)) {\n    return(NA)\n  }\n  \n  out &lt;- evd::dgev(\n    x = y,\n    loc = mu,\n    scale = sigma,\n    shape = xi,\n    log = TRUE\n  ) |&gt;\n    sum()\n  \n  prior_likelihood &lt;- priors$location(par[1]) +\n    priors$scale(par[2]) +\n    priors$shape(par[3]) +\n    priors$trend(par[4])\n  \n  out &lt;- out + prior_likelihood\n  \n  -out\n}"
  },
  {
    "objectID": "posts/gaussian-copula/index.html",
    "href": "posts/gaussian-copula/index.html",
    "title": "A Gentle Introduction: The Gaussian Copula",
    "section": "",
    "text": "This post is a continuation of last week’s blog post that introduces Copulas in Stan. This post will introduce the Gaussian copula, some of its positives and negatives, and how to code it up in Stan with an example.\nOther posts in this series:"
  },
  {
    "objectID": "posts/gaussian-copula/index.html#the-gaussian-copula",
    "href": "posts/gaussian-copula/index.html#the-gaussian-copula",
    "title": "A Gentle Introduction: The Gaussian Copula",
    "section": "The Gaussian Copula",
    "text": "The Gaussian Copula\n\nHigh-Level Summary\nModeling with the Gaussian copula can be summarised in a few steps:\n\nLet \\(\\mathbf X = (X_1, \\dots, X_D)\\) be a multivariate random variable with marginal distribution functions \\(F_i\\)\nEach \\(F_i\\left(X_i \\vert \\theta_i\\right)\\) converts the data, \\(X_i\\), to uniformly distributed variables, \\(u_i\\).\nUse the standard normal quantile function to convert each \\(u_i\\) to \\(z_i\\), \\(z_i = \\Phi^{-1}(u_i)\\)\nModel the dependencies using a correlation matrix (or precision matrix if you make sure its inverse is a correlation matrix)\n\n\n\nA Tiny Bit of Math\nMore formally, the multivariate CDF of \\(\\mathbf X\\) is written\n\\[\nH(\\mathbf X) = \\Phi_\\Sigma\\left( \\Phi^{-1}(F_1(X_1 \\vert \\theta_1)), \\dots, \\Phi^{-1}(F_D(X_D \\vert \\theta_D))  \\vert \\Sigma \\right)\n\\]\nwhere:\n\n\\(H(\\mathbf X)\\) is the joint cumulative distribution function (CDF) of the collection of random variables \\(\\mathbf X\\).\n\\(F_i(X_i \\vert \\theta_i)\\) are the marginal CDFs of each variate.\n\\(\\Phi^{-1}\\) is the inverse of the standard normal CDF.\n\\(\\Phi_\\Sigma\\) is the CDF of the multivariate normal distribution with mean vector \\(\\mathbf 0\\) and correlation matrix \\(\\Sigma\\).\n\nWriting out the multivariate density, we get\n\\[\n\\begin{aligned}\nh(\\mathbf X) &= c\\left(F_1(X_1 \\vert \\theta_1),  \\dots, F_D(X_D \\vert \\theta_D)\\right) \\prod_{i=1}^D f_i(X_i \\vert \\theta_i)\\\\\n&=\\frac{f_\\Sigma(z_1, \\dots, z_D \\vert \\Sigma)}{\\phi(z_1, \\dots, z_D)} \\prod_{i=1}^D f_i(X_i \\vert \\theta_i) \\\\\nz_i &= \\Phi^{-1}(u_i) \\\\\nu_i &= F_i(X_i \\vert \\theta_i)\n\\end{aligned}\n\\]\nwhere:\n\n\\(h(\\mathbf X)\\) is the joint density corresponding to the CDF, \\(H(\\mathbf X)\\).\n\\(f_\\Sigma\\) is the density of the multivariate Gaussian distribution with mean vector \\(\\mathbf 0\\) and correlation matrix \\(\\Sigma\\).\n\\(\\phi\\) is the density of the multivariate standard normal distribution.\n\\(f_i(X_i \\vert \\theta_i)\\) are the marginal densities of each variate.\n\n\n\n\n\n\n\nDeriving the Joint Density Using the Chain Rule and the Inverse Function Theorem\n\n\n\nWe start with the joint cumulative distribution function (CDF):\n\\[\nH(\\mathbf{X}) = \\Phi_\\Sigma\\left( \\Phi^{-1}(F_1(X_1 \\vert \\theta_1)), \\dots, \\Phi^{-1}(F_D(X_D \\vert \\theta_D)) \\mid \\Sigma \\right)\n\\]\nTo obtain the joint density \\(h(\\mathbf{X})\\), we are basically applying the chain rule to three functions:\n\\[\n\\begin{aligned}\n\\frac{d}{dx} [f(g(h(x)))] &= f'(g(h(x))) \\cdot g'(h(x)) \\cdot h'(x)\n\\end{aligned}\n\\]\nwhere \\(f\\) is our multivariate Gaussian, \\(g\\) is the standard normal quantile function and \\(h\\) corresponds to each of the marginal CDFs. The first and last parts should be pretty clear, and we can get the derivative of the quantile function with the inverse function theorem\n\\[\n\\left(\\Phi^{-1}\\right)'(u) = \\frac{1}{\\Phi'(\\Phi^{-1}(u))} = \\frac{1}{\\phi(\\Phi^{-1}(u))},\n\\]\nwhere \\(\\phi(z)\\) is the standard normal pdf. Knowing this, we can write out the density as\n\\[\n\\begin{aligned}\nh(\\mathbf{X}) &= f_{\\Sigma}(z_1, \\dots, z_D \\vert \\Sigma) \\prod_{i=1}^D \\frac{1}{\\phi(z_i)} \\prod_{i=1}^D f_i(X_i \\vert \\theta_i) \\\\\n&= \\frac{f_\\Sigma(z_1, \\dots, z_D \\vert \\Sigma)}{\\phi(z_1, \\dots, z_D)} \\prod_{i=1}^D f_i(X_i \\vert \\theta_i),\n\\end{aligned}\n\\]\nwhere we used the shorthand \\(\\phi(z_1, \\dots, z_d) = \\prod_{i=1}^D \\frac{1}{\\phi(z_i)}\\), to mean the density of a multivariate standard normal distribution.\n\n\nIn practice we (like lumberjacks) love logs, so we write:\n\\[\n\\begin{aligned}\n\\log h(\\mathbf X) &= \\log f_\\Sigma\\left( z_1, \\dots, z_D \\vert \\Sigma \\right) - \\log \\phi(z_1, \\dots, z_D) + \\sum_{i=1}^D \\log f_i(X_i \\vert \\theta_i) \\\\\nz_i &= \\Phi^{-1}(u_i) \\\\\nu_i &= F_i(X_i \\vert \\theta_i)\n\\end{aligned}\n\\]\nIf the correlation matrix, \\(\\Sigma\\), is equal to a diagonal matrix, this will just reduce to the i.i.d. copula mentioned in the previous post."
  },
  {
    "objectID": "posts/gaussian-copula/index.html#sampling-the-data",
    "href": "posts/gaussian-copula/index.html#sampling-the-data",
    "title": "A Gentle Introduction: The Gaussian Copula",
    "section": "Sampling the Data",
    "text": "Sampling the Data\n\nIn words\nTo sample from this data-generating process we\n\nGenerate \\(\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\).\nInduce desired correlation structure using the Cholesky factor, \\(\\mathbf{L}\\), of the correlation matrix: \\(\\mathbf{Z}' = \\mathbf{L} \\mathbf{Z}\\).\nApply the standard normal CDF, \\(\\Phi\\), to obtain uniform random variables, \\(\\mathbf{U} = \\Phi(\\mathbf{Z}')\\).\nTransform the uniform random variables to the desired marginal distributions using the inverse CDF of the exponential distribution: \\(\\mathbf{X} = F^{-1}_{\\text{exp}}(\\mathbf{U} \\vert \\lambda)\\).\n\n\n\nIn code\n\n\nCode\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(patchwork)\nlibrary(gt)\ntheme_set(bggjphd::theme_bggj())\n\n\n\n\nCode\nn_obs &lt;- 50\nrho &lt;- 0.6\nlambda1 &lt;- 2\nlambda2 &lt;- 4\nsigma &lt;- matrix(\n  c(\n    1, rho,\n    rho, 1\n  ),\n  nrow = 2\n)\nL &lt;- chol(sigma)\nset.seed(1)\nZ &lt;- matrix(rnorm(n = n_obs * 2), nrow = 2)\nZ &lt;- t(L %*% Z)\n\nd &lt;- tibble(\n  z1 = Z[, 1],\n  z2 = Z[, 2],\n  time = seq_len(n_obs)\n)  |&gt; \n  pivot_longer(\n    c(-time), \n    names_to = \"variable\", \n    names_transform = parse_number,\n    values_to = \"z\"\n  ) |&gt; \n  inner_join(\n    tibble(\n      variable = c(1, 2),\n      lambda = c(lambda1, lambda2)\n    )\n  ) |&gt; \n  mutate(\n    u = pnorm(z),\n    y = qexp(u, rate = lambda)\n  )\n\n\n\n\nCode\nd |&gt; \n  select(-lambda) |&gt; \n  pivot_longer(c(z, u, y)) |&gt; \n  pivot_wider(names_from = variable, names_prefix = \"v\") |&gt; \n  mutate(\n    name = fct_relevel(name, \"z\", \"u\") |&gt; \n      fct_recode(\n        \"Gaussian\" = \"z\",\n        \"Uniform\" = \"u\",\n        \"Exponential\" = \"y\"\n      )\n  ) |&gt; \n  group_by(n2 = name) |&gt; \n  group_map(\n    \\(data, ...) {\n      data  |&gt; \n        ggplot(aes(v1, v2)) +\n        geom_density_2d_filled(alpha = 0.5) +\n        geom_point(size = 1.4) +\n        scale_x_continuous(\n          expand = c(0, 0)\n        ) +\n        scale_y_continuous(\n          expand = c(0, 0)\n        ) +\n        theme(legend.position = \"none\") +\n        labs(\n          subtitle = unique(data$name),\n          x = expression(X[1]),\n          y = expression(X[2])\n        ) \n    }\n  ) |&gt; \n  wrap_plots(widths = c(1, 1, 1)) +\n  plot_annotation(\n    title = \"Going from Gaussian to Uniform to Exponential\"\n  )"
  },
  {
    "objectID": "posts/gaussian-copula/index.html#stan-model",
    "href": "posts/gaussian-copula/index.html#stan-model",
    "title": "A Gentle Introduction: The Gaussian Copula",
    "section": "Stan Model",
    "text": "Stan Model\nStan has built-in functions for the exponential lpdf and cdf, so the only thing we have to implement to fit our model is the Gaussian copula lpdf:\n\\[\n\\begin{aligned}\nc(u_1, \\dots, u_D \\vert \\Sigma) &= \\log f_\\Sigma(z_1, \\dots, z_D \\vert \\Sigma) - \\log \\phi(z_1, \\dots, z_D) \\\\\nz_i &= \\Phi^{-1}(u_i)\n\\end{aligned}\n\\]\nWe will code it up using the Cholesky factor of the correlation matrix\n\n\nCode\nreal gaussian_copula_lpdf(vector u, matrix L) {\n  int D = num_elements(u);\n  vector[D] z = inv_Phi(u);\n  return multi_normal_cholesky_lpdf(z | rep_vector(0, D), L) - normal_lpdf(z | 0, 1);\n}\n\n\nTo perform posterior predictive checks we’ll also have to define the exponential quantile function\n\\[\nQ(u) = -\\frac{\\ln(1 - u)}{\\lambda},\n\\]\nand use it in the generated quantities block.\n\n\nCode\nfunctions {\n  real exponential_icdf(real u, real lambda) {\n    return -log(1 - u) / lambda;\n  }\n}\n\n...\n...\n\ngenerated quantities {\n  matrix[N, D] yrep;\n\n  {\n    matrix[N, D] Z_rep;\n    matrix[N, D] U_rep;\n\n    for (i in 1:N) {\n      Z_rep[i, ] = to_row_vector(multi_normal_cholesky_rng(rep_vector(0, D), L));\n      for (j in 1:D) {\n        U_rep[i, j] = Phi(Z_rep[i, j]);\n        yrep[i, j] = exponential_icdf(U_rep[i, j], lambda[j]);\n      }\n    }\n  }\n}\n\n\nThus, we can write up our complete Stan model as below.\n\n\nCode\nfunctions {\n  real gaussian_copula_lpdf(vector u, matrix L) {\n    int D = num_elements(u);\n    vector[D] z = inv_Phi(u);\n    return multi_normal_cholesky_lpdf(z | rep_vector(0, D), L) - normal_lpdf(z | 0, 1);\n  }\n\n  real exponential_icdf(real u, real lambda) {\n    return -log(1 - u) / lambda;\n  }\n\n}\ndata {\n  int&lt;lower = 0&gt; N;\n  int&lt;lower = 0&gt; D;\n  matrix[N, D] X;\n}\n\nparameters {\n  vector[D] lambda;\n  cholesky_factor_corr[D] L;\n}\n\nmodel {\n  matrix[N, D] U;\n  for (i in 1:N) {\n    for (j in 1:D) {\n      target += exponential_lpdf(X[i, j] | lambda[j]);\n      U[i, j] = exponential_cdf(X[i, j] | lambda[j]);\n    }\n    target += gaussian_copula_lpdf(to_vector(U[i, ]) | L);\n  }\n  \n  target += lkj_corr_cholesky_lpdf(L | 1.0);\n}\n\ngenerated quantities {\n  corr_matrix[D] Sigma = multiply_lower_tri_self_transpose(L);\n  matrix[N, D] yrep;\n\n  {\n    matrix[N, D] Z_rep;\n    matrix[N, D] U_rep;\n\n    for (i in 1:N) {\n      Z_rep[i, ] = to_row_vector(multi_normal_cholesky_rng(rep_vector(0, D), L));\n      for (j in 1:D) {\n        U_rep[i, j] = Phi(Z_rep[i, j]);\n        yrep[i, j] = exponential_icdf(U_rep[i, j], lambda[j]);\n      }\n    }\n  }\n}"
  },
  {
    "objectID": "posts/gaussian-copula/index.html#sampling-from-the-posterior",
    "href": "posts/gaussian-copula/index.html#sampling-from-the-posterior",
    "title": "A Gentle Introduction: The Gaussian Copula",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\nPrepare the data and sample from the model.\n\n\nCode\nX &lt;- d |&gt;\n  select(time, variable, y) |&gt; \n  pivot_wider(names_from = variable, values_from = y) |&gt; \n  select(-time) |&gt; \n  as.matrix()\n\nstan_data &lt;- list(\n  X = X,\n  N = nrow(X),\n  D = ncol(X)\n)\n\nexample1 &lt;- cmdstan_model(here::here(\"posts\", \"gaussian-copula\", \"stan\", \"example1.stan\"))\n\nresult &lt;- example1$sample(\n  data = stan_data,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 1000,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\n\nBelow, we can see that we’ve come pretty close to the correct parameters.\n\n\nCode\nresult$summary(c(\"lambda\", \"Sigma[1,2]\")) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlambda[1]\n1.71\n1.70\n0.23\n0.22\n1.36\n2.10\n1.00\n2,367.64\n2,733.44\n\n\nlambda[2]\n4.65\n4.61\n0.64\n0.63\n3.68\n5.75\n1.00\n2,316.02\n2,465.39\n\n\nSigma[1,2]\n0.58\n0.59\n0.09\n0.09\n0.40\n0.71\n1.00\n2,334.49\n2,343.22\n\n\n\n\n\n\n\n\n\nCode\nmcmc_trace(result$draws(), pars = c(\"lambda[1]\", \"lambda[2]\", \"Sigma[1,2]\"))\n\n\n\n\n\n\n\n\n\nWe can also run a basic posterior predictive check\n\nCode\nyrep &lt;- result$draws(\"yrep\", format = \"matrix\")\ny &lt;- as.numeric(X)\n\nppc_dens_overlay(y = y[seq(1, 50)], yrep = yrep[1:100, seq(1, 50)]) + ggtitle(expression(X[1]))\nppc_dens_overlay(y = y[seq(51, 100)], yrep = yrep[1:100, seq(51, 100)]) + ggtitle(expression(X[2]))"
  },
  {
    "objectID": "posts/plotly-animations/index.html",
    "href": "posts/plotly-animations/index.html",
    "title": "Making interactive animations with ggplotly",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(glue)\nlibrary(mgcv)\n# Some personal packages I've written with\n# plot themes, functions for easily\n# working with Icelandic data etc.\nlibrary(hagstofa)\nlibrary(metill)\nlibrary(visitalaneysluverds)\nlibrary(bggjphd)\n\ntheme_set(theme_bggj())"
  },
  {
    "objectID": "posts/plotly-animations/index.html#simple-example",
    "href": "posts/plotly-animations/index.html#simple-example",
    "title": "Making interactive animations with ggplotly",
    "section": "Simple example",
    "text": "Simple example\nIf you assign a variable to the frame aesthetic when making ggplot2 plots, ggplotly() will automatically create an animation slider and button using that variable.\n\n\nCode\np &lt;- mtcars |&gt; \n  ggplot(aes(x = wt, y = mpg, frame = cyl)) +\n  geom_point() +\n  labs(\n    title = \"Example with mtcars\"\n  )\n\nggplotly(p)"
  },
  {
    "objectID": "posts/plotly-animations/index.html#effect-of-gamma-on-gamm-smoothness",
    "href": "posts/plotly-animations/index.html#effect-of-gamma-on-gamm-smoothness",
    "title": "Making interactive animations with ggplotly",
    "section": "Effect of gamma on GAMM smoothness",
    "text": "Effect of gamma on GAMM smoothness\nThe gam() function from the package mgcv package has an input called gamma. The documentation has this to say:\n\nIncrease this beyond 1 to produce smoother models. gamma multiplies the effective degrees of freedom in the GCV or UBRE/AIC. coden/gamma can be viewed as an effective sample size in the GCV score, and this also enables it to be used with REML/ML. Ignored with P-RE/ML or the efs optimizer.\n\nIn the code below I fit multiple mgcv::gam() models with varying gamma parameters. I then use gamma as the frames for a ggplotly animation to see its effects on smoothness.\n\n\nCode\nwt_range &lt;- range(mtcars$wt)\nwt_seq &lt;- seq(wt_range[1], wt_range[2], length.out = 70)\n\npred_dat &lt;- mtcars |&gt; \n  crossing(\n    gamma = exp(seq(-4, 1, length.out = 15))\n  ) |&gt; \n  nest(data = -gamma) |&gt; \n  mutate(\n    model = map2(gamma, data, ~ gam(mpg ~ s(wt), data = .y, gamma = .x, method = \"REML\")),\n    preds = map(model, broom::augment, newdata = tibble(wt = wt_seq))\n  ) |&gt; \n  unnest(preds) |&gt; \n  select(gamma, wt, .fitted)\n\np &lt;- ggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_line(\n    data = pred_dat,\n    aes(x = wt, y = .fitted, frame = gamma)\n  ) +\n  labs(\n    title = \"Animating different parameter values\"\n  )\n\nggplotly(p)"
  },
  {
    "objectID": "posts/plotly-animations/index.html#about-the-data",
    "href": "posts/plotly-animations/index.html#about-the-data",
    "title": "Making interactive animations with ggplotly",
    "section": "About the Data",
    "text": "About the Data\nI won’t go into detail about how I created the dataset. Let’s just imagine that we are handed data on total and salaried income for Icelanders by age-group and income bracket (quantile).\n\n\nCode\nd |&gt; \n  glimpse()\n\n\nRows: 2,480\nColumns: 8\n$ year         &lt;dbl&gt; 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 1991, 199…\n$ age          &lt;chr&gt; \"16 - 24 Years Old\", \"16 - 24 Years Old\", \"16 - 24 Years …\n$ quantile     &lt;dbl&gt; 10, 20, 30, 40, 50, 60, 70, 80, 90, 99, 10, 20, 30, 40, 5…\n$ type         &lt;chr&gt; \"Total Income\", \"Total Income\", \"Total Income\", \"Total In…\n$ mean_income  &lt;dbl&gt; 2459.058, 2459.058, 2459.058, 2459.058, 2459.058, 2459.05…\n$ group_income &lt;dbl&gt; 676.3329, 992.4450, 1304.8814, 1654.0750, 2043.7015, 2470…\n$ group_change &lt;dbl&gt; 0.013355780, 0.011152832, 0.016421043, 0.012773270, 0.012…\n$ mean_change  &lt;dbl&gt; 0.002164807, 0.002164807, 0.002164807, 0.002164807, 0.002…"
  },
  {
    "objectID": "posts/plotly-animations/index.html#plotting",
    "href": "posts/plotly-animations/index.html#plotting",
    "title": "Making interactive animations with ggplotly",
    "section": "Plotting",
    "text": "Plotting\n\nLabels and tooltips\nThe first step is to create fancy tooltips to pipe into plotly. These can be assigned to aesthetics (I use the aesthetic name text), and then we tell ggplotly() by writing, for example, ggplotly(tooltip = \"text\"). I’ve output examples of how the formatted tooltip text looks below.\n\n\nCode\nplot_dat &lt;- d |&gt; \n  mutate(\n    label = ifelse((quantile == 10) & (age == \"16 - 24 Years Old\") & (type == \"Salary Income\"),\n                   \"Change of mean income for age group\", \n                   NA_character_),\n    text = glue(str_c(\n      \"&lt;b&gt;Quantile: {quantile}%&lt;/b&gt;\", \"\\n\",\n      \"Year: {year}\", \"\\n\",\n      \"Income (yearly): {isk(group_income * 1e3, scale = 1e-6)}\", \"\\n\",\n      \"Income (monthly): {isk(group_income/12 * 1e3, scale = 1e-3)}\"\n    ))\n  )\n\nplot_dat |&gt; \n  slice(1:2) |&gt; \n  pull(text)\n\n\n&lt;b&gt;Quantile: 10%&lt;/b&gt;\nYear: 1991\nIncome (yearly): 1 m.kr\nIncome (monthly): 56 þús.kr\n&lt;b&gt;Quantile: 20%&lt;/b&gt;\nYear: 1991\nIncome (yearly): 1 m.kr\nIncome (monthly): 83 þús.kr\n\n\n\n\nggplot\nNext we create the ggplot() object. It will not look very good, since all the frames are plotted together.\nThings to note here:\n\nThe percent change variables are between 0 and 1, i.e. percentages. I want to use a log-scale such that -50% will be as far away from 0 as +100%. To do this I add 1 to the percentages and use a custom labeling function, labels = function(x) percent(x - 1), in scale_y_continuous(). This means that no change in the y-variable is represented by 1 in the data but 0 in the plot.\nI capitalized the year variable beforehand. I could change the labeling of the animation slider after the fact, but I find this to be quicker.\nThere are some differences between ggplot and ggplotly output when it comes to text sizes and aspect ratios, so it’s good to tune the sizing after the animation is ready.\n\n\n\nCode\np &lt;- plot_dat |&gt; \n  rename(Year = year) |&gt; \n  ggplot(aes(quantile, group_change + 1, frame = Year, text = text)) + \n  geom_hline(yintercept = 1, lty = 2, alpha = 0.4, linewidth = 0.4) +\n  geom_hline(\n    aes(yintercept = mean_change + 1, frame = Year),\n    alpha = 0.5,\n    lty = 3,\n    colour = \"#e41a1c\"\n  ) +\n  geom_text(\n    aes(\n      x = 45, \n      y = (mean_change + 1) * 1.15,\n      label = label\n    ),\n    hjust = 0, vjust = 1,\n    colour = \"#e41a1c\"\n  ) +\n  geom_point() +\n  geom_segment(aes(xend = quantile, yend = 1), lty = 2, alpha = 0.5) +\n  scale_x_continuous(\n    breaks = c(seq(10, 90, by = 10), 99),\n    labels = label_number(suffix = \"%\")\n  ) +\n  scale_y_continuous(\n    labels = function(x) percent(x - 1),\n    trans = \"log10\"\n  ) +\n  facet_grid(cols = vars(age), rows = vars(type)) +\n  labs(\n    x = \"Income Quantile\",\n    y = \"Percent change since 1990\",\n    title = \"How has total and salary income changed by age-groups and income-brackets since 1990? (Adjusted for inflation)\"\n  )\n\np\n\n\n\n\n\n\n\n\n\n\n\nggplotly\nNext we use this ggplot() object as an input into ggplotly(), letting it know that the tooltip can be found in the aesthetic we called text.\n\n\nCode\nggplotly(p, tooltip = \"text\")"
  },
  {
    "objectID": "posts/icar_gev/index.html",
    "href": "posts/icar_gev/index.html",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "",
    "text": "Note: I’m still working on this post, but I thought I might put it out there while I work on it.\nCode\nlibrary(bggjphd)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(here)\nlibrary(gt)\nlibrary(skimr)\nlibrary(arrow)\nlibrary(leafsync)\nlibrary(sf)\nlibrary(mapview)\ntheme_set(theme_bggj())"
  },
  {
    "objectID": "posts/icar_gev/index.html#extreme-value-statistics",
    "href": "posts/icar_gev/index.html#extreme-value-statistics",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "Extreme Value Statistics",
    "text": "Extreme Value Statistics\nUp until now I have mostly been working with the generalized extreme value distribution. The cumulative distribution function of the Generalized Extreme Value distribution is\n\\[\n\\mathrm{GEV}(y \\vert \\mu, \\sigma, \\xi) = \\begin{cases}\n\\begin{aligned}\n&e^{- \\left(1 + \\xi \\frac{y - \\mu}{\\sigma}\\right)_+^{-1/\\xi}}, \\quad &\\xi \\neq 0 \\\\\n&e^{-e^{- \\frac{y - \\mu}{\\sigma}}}, \\qquad &\\xi = 0\n\\end{aligned}\n\\end{cases}\n\\]\nThe log-likelihood of the GEV distribution is\n\\[\n\\ell(\\mu, \\sigma, \\xi) = - n\\log\\sigma - (1 + \\frac{1}{\\xi}) \\sum_{i=1}^{n}{\\log\\left(1 + \\xi\\left[\\frac{z_i - \\mu}{\\sigma} \\right]\\right)} - \\sum_{i=1}^{n}{\\left(1 + \\xi \\left[ \\frac{z_i - \\mu}{\\sigma} \\right]\\right)}^{-1/\\xi},\n\\]\nprovided that \\(1 + \\xi\\left( \\frac{z_i - \\mu}{\\sigma} \\right) &gt; 0\\). Instead of \\(\\mu\\) I will be using the parameter \\(\\mu_t\\) where\n\\[\n\\mu_t = \\mu_0 \\cdot (1 + \\Delta(t - t_0)) = \\mu_0 + \\Delta(t - t_0) \\cdot \\mu_0.\n\\]\nWe thus estimate two parameters that are related to the location of the GEV distribution, \\(\\mu_0\\) and \\(\\Delta\\)."
  },
  {
    "objectID": "posts/icar_gev/index.html#spatial-statistics",
    "href": "posts/icar_gev/index.html#spatial-statistics",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "Spatial Statistics",
    "text": "Spatial Statistics\nIn our formulation, the GEV distribution has four parameters:\n\n\\(\\mu\\): Location\n\\(\\sigma\\): Scale\n\\(\\xi\\): Shape\n\\(\\Delta\\): Trend\n\nYou could also just say that there are three parameters and that we’re using a linear model to allow a trend in the location parameter, but we’ll keep this wording for now.\nI will be fitting a GEV distribution to each area in the CEDA Archived data I’ve written about before. It’s logical to assume that these areas are not completely independent and that there is some sort of spatial dependency, i.e. that all other things being equal; areas that are near each other are more similar that areas that are far away from each other.\nWe could apply a model structure based on the distance between points, but we will end up having a lot of areas (around 40,000 or so), so we need a spatial model that allows for sparsity in the spatial dependence.\nOne way to do this is to model the areas as jointly multivariate normal distributed with a sparse precision matrix. Precision matrices can be better than covariance matrices for this type of modeling since the off-diagonal elements in a precision matrix stand for conditional dependencies.\nMore formally, let \\(\\mathbf x\\) be multivariate normal distributed with mean \\(\\boldsymbol \\mu\\) and semi positive definite precision matrix \\(\\mathbf Q\\). Then for \\(i \\neq j\\)\n\\[\nx_i \\perp x_j \\vert \\mathbf x_{-ij} \\iff Q_{ij} = 0.\n\\]\nThis means that any two elements in \\(\\mathbf x\\) are conditionally independent conditional on the other elements of \\(\\mathbf x\\) if and only if the corresponding value in the precision matrix \\(\\mathbf Q\\) is zero. For further reading see Rue and Held (2005).\n\nRue, Havard, and Leonhard Held. 2005. Gaussian Markov Random Fields: Theory and Applications. CRC Press.\nWe can easily use this to our advantage so that our precision matrix becomes very sparse. For example we might set \\(Q_{ij} \\neq 0\\) only if \\(i\\) and \\(j\\) are neighbors on our map.\nThe rest of this chapter on spatial statistics is based heavily on the paper by Morris et al. (2019).\n\nMorris, Mitzi, Katherine Wheeler-Martin, Dan Simpson, Stephen J. Mooney, Andrew Gelman, and Charles DiMaggio. 2019. “Bayesian Hierarchical Spatial Models: Implementing the Besag York Mollié Model in Stan.” Spatial and Spatio-Temporal Epidemiology 31 (November): 100301. https://doi.org/10.1016/j.sste.2019.100301.\n\nConditional Autoregression (CAR) Models\nIn a CAR prior, the prior distribution of \\(x_i\\) given its neighbors can be written\n\\[\nx_i \\vert x_{-i} \\sim \\mathrm{Normal}\\left( \\sum_{j\\in S_i} w_{ij}x_j, \\sigma^2 \\right),\n\\]\nwhere \\(x_{-i}\\) means every element of \\(x\\) except for \\(x_i\\), \\(S_i\\) is the set of neighbors of \\(x_i\\), and \\(w_{ij}\\) are weights.\nThis can be written as a multivariate normal variate with mean 0 and a precision matrix, \\(\\mathbf Q\\), which is defined as\n\\[\nQ = D(I - \\alpha A).\n\\]\nHere, D is a diagonal matrix with \\(D_{ii} = d_i\\) being equal to the number of neighbors of \\(x_i\\), \\(A\\) is the adjacency matrix\n\\[\nA_{ij} = 1 \\iff x_j \\in S_i,\n\\]\n\\(I\\) is the identity matrix, and \\(0 &lt; \\alpha &lt; 1\\) is a parameter that encodes the amount of spatial dependence. If \\(\\alpha = 0\\) then there is no spatial dependence and if \\(\\alpha = 1\\) we get what is called an ICAR model.\n\n\nIntrinsic Conditional Autoregression (ICAR) Models\nIn an ICAR model the spatial dependence parameter \\(\\alpha = 1\\) and so, the precision matrix, \\(Q\\), is singular since then\n\\[\nQ = D(I - A).\n\\]\nRecall that \\(D_{ii} = d_i\\) is equal to the number of neighbors of \\(x_i\\) and so the diagonal of \\(Q\\) will be equal to the sum of its off-diagonal elements. This can still be used as a prior, but we must take care since it is an improper prior.\nThe conditional distribution of \\(x_i\\) given all other observations is\n\\[\nx_i \\vert x_{-i} \\sim \\mathrm{Normal}\\left( \\frac{ \\sum_{j\\in S_i}{x_j}}{d_i}, \\frac{\\sigma_i^2}{d_i}\\right).\n\\]\nIf we specify that \\(x\\) as mean 0 and variance 1, then the joint distribution of \\(x\\) becomes\n\\[\nx \\sim \\exp\\left(-\\frac12 \\sum_{j\\in S_i}{(x_i - x_j)^2}\\right).\n\\]\nWe can easily see that this is not proper, since any constant added to all of the \\(x_i\\)’s will give the same density. One way around this issue is to add the constraint \\(\\sum x_i = 0\\).\n\n\nBesag York Mollié (BYM) Model\nThe BYM model is composed of two different types of random effects:\n\nAn ICAR component \\(\\phi\\)\nAn iid non-spatial component \\(\\theta\\)\n\nHere, both \\(\\phi\\) and \\(\\theta\\) are assumed to follow normal distributions with means 0 and precision parameters \\(\\tau_\\phi\\) and \\(\\tau_\\theta\\). We can see that if \\(\\tau_\\phi\\) is estimated to be much higher than \\(\\tau_\\theta\\) then our data is implying more spatial than non-spatial dependence (and vice versa).\nOne difficulty in using this model is that apriori we want “fair” hyperpriors for the precision parameters, i.e. we do not want our prior to weight our model in the direction of more or less spatial dependence. One way to choose these priors is the formula from (L, D, and C 1995):\n\nL, Bernardinelli, Clayton D, and Montomoli C. 1995. “Bayesian Estimates of Disease Maps: How Important Are Priors?” Statistics in Medicine 14 (21-22). https://doi.org/10.1002/sim.4780142111.\n\\[\nsd(\\theta_i) = \\frac{1}{\\sqrt{\\tau_\\phi}} \\approx \\frac{1}{0.7\\sqrt{\\bar m \\tau_\\theta}} \\approx sd(\\phi_i),\n\\]\nwhere \\(\\bar m\\) is the average number of neighbors for each \\(x_i\\).\n\n\nBYM2 Model\nThe BYM2 model (Riebler et al. 2016) rewrites the hyperpriors from the BYM model in a way that follows the Penalized Complexity Priors framework (Simpson et al. 2015). The prior is rewritten so that a single scale parameter, \\(\\sigma\\), determines the variance of both components, and a mixing parameter, \\(\\rho\\), determines the amount of spatial/non-spatial random effect.\n\nRiebler, Andrea, Sigrunn H. Sørbye, Daniel Simpson, and Håvard Rue. 2016. “An Intuitive Bayesian Spatial Model for Disease Mapping That Accounts for Scaling.” August 2016. https://journals.sagepub.com/doi/full/10.1177/0962280216660421.\n\nSimpson, Daniel P., Håvard Rue, Thiago G. Martins, Andrea Riebler, and Sigrunn H. Sørbye. 2015. “Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors.” August 6, 2015. http://arxiv.org/abs/1403.4630.\nThe combined effects, \\(\\phi + \\theta\\), are thus rewritten as\n\\[\n\\sigma \\cdot \\left( \\sqrt{\\rho} \\cdot \\frac{\\phi^*}{\\sqrt s} + \\sqrt{1 - \\rho} \\cdot\\theta^* \\right),\n\\] where \\(0 \\leq \\rho \\leq 1\\) determines the amount of spatial/non-spatial error, \\(\\phi^*\\) is the ICAR model, \\(\\theta^*\\) is an iid random effect, \\(s\\) is the scaling factor for the neighborhood graph, \\(\\sigma \\geq 0\\) is the overall standard deviation of the combined random effects."
  },
  {
    "objectID": "posts/icar_gev/index.html#copulas",
    "href": "posts/icar_gev/index.html#copulas",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "Copulas",
    "text": "Copulas\nEven if we use the aforementioned models to allow spatial dependence between the paremeters in our GEV distributions, there is still one problem: We’re not modeling spatial distributions on the data level.\nThis means that our model as it is currently set up can allow spatial dependence in how often extreme precipitation happens in neighboring regions, but it does not allow for spatial dependence in when we observe extreme precipitation. To do this, we will need to apply copulas. I will not write about them here, but I have a post on the way about applying copulas to data with GEV margins."
  },
  {
    "objectID": "posts/icar_gev/index.html#subset-of-the-data",
    "href": "posts/icar_gev/index.html#subset-of-the-data",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "Subset of the data",
    "text": "Subset of the data\nFor this analysis, I will be subsetting the data to save time. I’m going to use 10.201 stations with X projections between 50 and 150, and Y projections between 100 and 200.\n\nStations\nTo be able to index the stations in my Stan program, I give the stations new names according to their row number. In the filtered dataset.\n\n\nCode\nmodel_stations &lt;- stations |&gt;\n  filter(\n    between(proj_x, 50, 150),\n    between(proj_y, 100, 200)\n  )\n\nnew_names &lt;- model_stations |&gt; \n  mutate(new_name = row_number()) |&gt; \n  distinct(station, new_name)\n\nmodel_stations |&gt; \n  skim()\n\n\n\n\nPrecipitation\n\n\nCode\nmodel_precip &lt;- precip |&gt;\n  semi_join(\n    model_stations,\n    by = join_by(station)\n  )\n\nmodel_precip |&gt; \n  skim()\n\n\n\n\nCode\np &lt;- model_precip |&gt; \n  filter(\n    station %in% sample(station, size = 3)\n  ) |&gt; \n  mutate(\n    period = 1 + (year &gt;= 2020) + (year &gt;= 2060),\n    period = c(\"1980 - 2000\", \"2020 - 2040\", \"2060 - 2080\")[period],\n    station = str_c(\"Station\\n\", station)\n  ) |&gt; \n  ggplot(aes(year, precip)) +\n  geom_point() +\n  geom_segment(\n    aes(\n      yend = 0, xend = year\n    ),\n    lty = 2,\n    alpha = 0.5,\n    linewidth = 0.6\n  ) +\n  facet_grid(\n    cols = vars(period),\n    rows = vars(station),\n    scales = \"free_x\"\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"An example of extreme value observations\",\n    subtitle = \"Annual values of maximum daily precipitation for a sample of stations in the data\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/ts_extreme_plot.png\",\n  width = 8, height = 0.621 * 8, scale = 1.3\n)"
  },
  {
    "objectID": "posts/icar_gev/index.html#prepare-the-data-for-stan",
    "href": "posts/icar_gev/index.html#prepare-the-data-for-stan",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "Prepare the data for Stan",
    "text": "Prepare the data for Stan\n\n\nCode\nprecip_matrix &lt;- model_precip |&gt;\n  pivot_wider(names_from = station, values_from = precip) |&gt;\n  column_to_rownames(\"year\") |&gt;\n  as.matrix()\n\nN_stations &lt;- ncol(precip_matrix)\nN_years &lt;- nrow(precip_matrix)\n\n\n\n\nCode\nedges &lt;- twelve_neighbors |&gt;\n  filter(\n    type %in% c(\"e\", \"n\", \"w\", \"s\")\n  ) |&gt;\n  inner_join(\n    model_stations,\n    by = join_by(station)\n  ) |&gt;\n  semi_join(\n    model_stations,\n    by = join_by(neighbor == station)\n  ) |&gt;\n  select(station, neighbor) |&gt; \n  update_names(station) |&gt; \n  update_names(neighbor)\n\nN_neighbors = nrow(edges)\nnode1 &lt;- edges$station\nnode2 &lt;- edges$neighbor\n\n\n\n\nCode\nnbs &lt;- edges |&gt;\n  filter(neighbor &gt; station) |&gt;\n  rename(node1 = station, node2 = neighbor)\n\nN &lt;- nrow(model_stations)\n\nadj.matrix &lt;- sparseMatrix(i = nbs$node1, j = nbs$node2, x = 1, symmetric = TRUE)\n# The ICAR precision matrix (note! This is singular)\nQ &lt;- Diagonal(N, rowSums(adj.matrix)) - adj.matrix\n# Add a small jitter to the diagonal for numerical stability (optional but recommended)\nQ_pert &lt;- Q + Diagonal(N) * max(diag(Q)) * sqrt(.Machine$double.eps)\n\n# Compute the diagonal elements of the covariance matrix subject to the\n# constraint that the entries of the ICAR sum to zero.\n# See the inla.qinv function help for further details.\nQ_inv &lt;- inla.qinv(Q_pert, constr = list(A = matrix(1, 1, N), e = 0))\n\n# Compute the geometric mean of the variances, which are on the diagonal of Q.inv\nscaling_factor &lt;- exp(mean(log(diag(Q_inv))))\n\n\n\n\nCode\nstan_data &lt;- list(\n  N_stations = N_stations,\n  N_years = N_years,\n  precip = precip_matrix,\n  N_neighbors = N_neighbors,\n  node1 = node1,\n  node2 = node2,\n  scaling_factor = scaling_factor\n)"
  },
  {
    "objectID": "posts/icar_gev/index.html#bym2-parameters",
    "href": "posts/icar_gev/index.html#bym2-parameters",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "BYM2 Parameters",
    "text": "BYM2 Parameters\n\n\nCode\nhere(\"posts\", \"icar_gev\", \"results\", \"bym_results.csv\") |&gt; \n  read_csv() |&gt; \n  mutate(\n    variable = str_c(\"&lt;b&gt;&\", variable, \";&lt;/sub&gt;&lt;/b&gt;\") |&gt; \n      str_replace(\"_\", \";&lt;sub&gt;&\")\n  ) |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Our BYM2 hyperparameters point to a large degree of spatial variation\"\n  ) |&gt; \n  fmt_markdown(\n    columns = variable\n  ) |&gt; \n  fmt_number(\n    columns = mean:rhat,\n    decimals = 3\n  ) |&gt; \n  fmt_number(\n    columns = ess_bulk:ess_tail,\n    decimals = 0\n  ) |&gt; \n  tab_options(table.width = pct(100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur BYM2 hyperparameters point to a large degree of spatial variation\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nσψ\n0.072\n0.072\n0.001\n0.001\n0.070\n0.074\n1.012\n137\n387\n\n\nμψ\n2.133\n2.133\n0.001\n0.001\n2.131\n2.135\n1.001\n5,029\n3,417\n\n\nρψ\n0.998\n0.998\n0.001\n0.001\n0.997\n0.999\n1.000\n3,161\n3,218\n\n\nστ\n0.102\n0.102\n0.002\n0.002\n0.098\n0.106\n1.015\n381\n973\n\n\nμτ\n−0.923\n−0.923\n0.001\n0.001\n−0.926\n−0.921\n1.000\n4,130\n3,507\n\n\nρτ\n0.997\n0.998\n0.001\n0.001\n0.996\n0.999\n1.001\n3,192\n2,903\n\n\nσφ\n0.358\n0.358\n0.009\n0.009\n0.343\n0.372\n1.008\n401\n730\n\n\nμφ\n0.341\n0.341\n0.004\n0.004\n0.335\n0.347\n1.001\n3,728\n3,049\n\n\nρφ\n0.996\n0.997\n0.001\n0.001\n0.994\n0.998\n1.002\n1,995\n2,677\n\n\nσγ\n0.332\n0.333\n0.011\n0.011\n0.313\n0.351\n1.029\n105\n208\n\n\nμγ\n1.438\n1.438\n0.012\n0.012\n1.419\n1.458\n1.001\n2,517\n3,053\n\n\nργ\n0.996\n0.996\n0.002\n0.001\n0.993\n0.998\n1.000\n3,823\n2,875"
  },
  {
    "objectID": "posts/icar_gev/index.html#spatial-distribution",
    "href": "posts/icar_gev/index.html#spatial-distribution",
    "title": "Spatially Dependent Generalized Extreme Value Parameters",
    "section": "Spatial Distribution",
    "text": "Spatial Distribution\n\nUnconstrained Scale\n\n\n\nConstrained Scale"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html",
    "href": "posts/gev_copula_ar1/index.html",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "",
    "text": "Code\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(bggjphd)\ntheme_set(theme_bggj())"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#copulas",
    "href": "posts/gev_copula_ar1/index.html#copulas",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Copulas",
    "text": "Copulas\nIn the book Elements of Copula Modeling with R, Hofert et al. (2018) define a copula as\n\nHofert, Marius, Ivan Kojadinovic, Martin Mächler, and Jun Yan. 2018. Elements of Copula Modeling with R. Use R! Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-89635-9.\n\na multivariate distribution function with standard uniform univariate margins, that is, U(0, 1) margins.\n\nSo, a Copula is any multivariate distribution whose margins is U(0,1 ) distributed. A simple example is the independence copula\n\\[\n\\Pi(u) = \\prod_{j=1}^{d}{u_j}, \\quad \\mathbf U \\in [0, 1]^d.\n\\]\nThe central theorem of copula theory is Sklar’s theorem (Sklar 1996) [the original paper is from 1959]. The theorem basically states that for any d-dimensional distribution function, \\(H\\), with univariate margins \\(F_1, \\dots, F_d\\), there exists a d-dimensional copula \\(C\\) such that\n\nSklar, A. 1996. “Random Variables, Distribution Functions, and Copulas: A Personal Look Backward and Forward.” Lecture Notes-Monograph Series 28: 1–14. https://www.jstor.org/stable/4355880.\n\\[\nH(\\mathbf x) = C(F_1(x_1), \\dots, F_d(x_d)).\n\\]\nAlternatively, we can write\n\\[\nC(\\mathbf u) = H(F_1^{-1}(u_1), \\dots, F_d^{-1}(u_d))\n\\]\n\nGaussian Copula\nThe Gaussian family of copulas can be written\n\\[\nC(\\mathbf u | R) = \\Phi_d\\left(\\Phi^{-1}(u_1), \\dots, \\Phi^{-1}(u_d) \\vert R \\right), \\quad \\mathbf u \\in [0, 1]^d,\n\\]\nwhere R is a \\(d\\times d\\) correlation matrix, \\(\\Phi_d\\) is the CDF of a d-dimensional multivariate Gaussian with mean \\(\\mathbf 0\\) and covariance matrix \\(R\\), and \\(\\Phi\\) is the CDF of a standard Gaussian.\nIts density is\n\\[\nc(\\mathbf u \\vert R) = \\frac{\\phi_d\\left(\\Phi^{-1}(u_1), \\dots, \\Phi^{-1}(u_d) \\vert R \\right)}{ \\prod_{j=1}^{d}{\\phi\\left( \\Phi^{-1}(u_j) \\right)}}, \\quad \\mathbf u \\in [0, 1]^d,\n\\]\nwhere \\(\\phi_d\\) is the density of the same multivariate Gaussian, and \\(\\phi\\) is the density of a standard gaussian.\n\nSampling from the Gaussian copula\nGiven the \\(d\\times d\\) correlation matrix, \\(R\\):\n\nCompute the Cholesky factor \\(L\\) of the correlation matrix, \\(R\\).\nSample \\(Z_1, \\dots, Z_d \\overset{iid}{\\sim}\\mathrm{Normal}(0, 1)\\).\nCompute \\(X = LZ\\).\nReturn \\(\\mathbf u = \\left(\\Phi(X_1), \\dots, \\Phi(X_d)\\right)\\)\n\nWe can then make the margins \\(u_1, \\dots, u_d\\) follow any distribution (f.ex. the GEV distribution) by applying its quantile function.\n\n\n\nExample\nLet’s sample from a two-dimensional process with GEV(8, 2, 0.05) margins and a Gaussian copula with correlation \\(\\rho = 0.7\\).\n\n\nCode\nR &lt;- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nset.seed(1)\nX &lt;- mvtnorm::rmvnorm(\n  n = 200,\n  sigma = R\n)\n\nX |&gt; \n  as_tibble() |&gt; \n  mutate(\n    id = row_number()\n  ) |&gt; \n  pivot_longer(c(-id), names_to = \"variable\", values_to = \"Z\") |&gt; \n  mutate(\n    U = pnorm(Z),\n    Y = evd::qgev(U, loc = 8, scale = 2, shape = 0.05)\n  ) |&gt; \n  pivot_longer(c(-id, -variable)) |&gt; \n  pivot_wider(names_from = variable, values_from = value) |&gt;\n  mutate(\n    name = fct_relevel(name, \"Z\", \"U\", \"Y\") |&gt; \n      fct_recode(\n        \"Z (Gaussian)\" = \"Z\",\n        \"U (Uniform)\" = \"U\",\n        \"Y (GEV)\" = \"Y\"\n      )\n  ) |&gt; \n  group_by(name2 = name) |&gt; \n  group_map(\n    function(data, ...) {\n      data |&gt; \n        ggplot(aes(V1, V2)) +\n        geom_density_2d_filled() +\n        geom_point(col = \"grey70\") +\n        coord_cartesian(expand = FALSE) +\n        theme(legend.position = \"none\") +\n        labs(\n          x = NULL,\n          y = NULL,\n          subtitle = data$name\n        )\n    }\n  ) |&gt; \n  wrap_plots() +\n  plot_annotation(\n    title = \"The stages in generating 2d GEV(8, 2, 0.05) data with a Gaussian correlation 0.7\"\n  )"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#data",
    "href": "posts/gev_copula_ar1/index.html#data",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Data",
    "text": "Data\nR scripts used for simulating the data\nThe simulated data have GEV margins and an AR(1) Gaussian copula. The steps in simulating the data are as follows.\n\nGenerate a correlation matrix, R, based on an AR(1) process\nGenerate multivariate normal variates with mean zero and covariance matrix R.\nTransform to GEV by applying normal CDF and GEV quantile functions\n\n\n1. Correlation Matrix\nFirst, a correlation matrix is generated that encodes an AR(1) process with correlation \\(\\rho\\). To create this correlation matrix, we specify the precision matrix, \\(Q\\), via\n\\[\nQ = \\frac{1}{1 - \\rho^2}\\begin{bmatrix}\n1 & -\\rho & \\dots & \\dots & \\vdots\\\\\n-\\rho & 1 + \\rho^2 & -\\rho & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & \\ddots &-\\rho & 1 + \\rho^2 & -\\rho \\\\\n\\vdots & \\dots & \\dots & -\\rho & 1\n\\end{bmatrix}.\n\\]\nThis gives us the correlation matrix, R, where\n\\[\nR = Q^{-1} = \\begin{bmatrix}\n1 & \\rho & \\rho^2 & \\dots & \\rho^n\\\\\n\\rho & 1 & \\rho & \\ddots & \\rho^{n-1} \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n\\rho^{n-1} & \\ddots &\\rho & 1 & \\rho \\\\\n\\rho^n & \\dots & \\dots & \\rho & 1\n\\end{bmatrix}.\n\\]\n\n\nCode\nmake_AR_cor_matrix_1d &lt;- function(n_id, rho = 0.5) {\n  P &lt;- matrix(\n    0, \n    nrow = n_id,\n    ncol = n_id\n  )\n  diag(P) &lt;- 1\n  for (i in seq(1, n_id - 1)) {\n    P[i, i + 1] &lt;- -rho\n  }\n  \n  for (i in seq(2, n_id)) {\n    P[i, i - 1] &lt;- -rho\n  }\n  \n  for (i in seq(2, n_id - 1)) {\n    P[i, i] &lt;- 1 + rho^2\n  }\n  \n  P &lt;- P / (1 - rho^2)\n  \n  P_cor &lt;- solve(P)\n  P_cor\n}\n\nn_id &lt;- 5\nrho &lt;- 0.5\n\nR &lt;- make_AR_cor_matrix_1d(n_id = n_id, rho = rho)\nR\n\n\n       [,1]  [,2] [,3]  [,4]   [,5]\n[1,] 1.0000 0.500 0.25 0.125 0.0625\n[2,] 0.5000 1.000 0.50 0.250 0.1250\n[3,] 0.2500 0.500 1.00 0.500 0.2500\n[4,] 0.1250 0.250 0.50 1.000 0.5000\n[5,] 0.0625 0.125 0.25 0.500 1.0000\n\n\n\n\n2. Generating Multivariate Normal Data\nWe then use the well-known fact that if\n\\[\n\\mathbf X \\sim \\mathrm{MVNorm}\\left(\\boldsymbol \\mu, \\Sigma\\right),\n\\]\nthen\n\\[\n\\boldsymbol X = \\boldsymbol \\mu +  L \\boldsymbol Z,\n\\]\nwhere \\(\\boldsymbol Z \\sim \\mathrm{Normal}(\\boldsymbol 0,  I)\\) and \\(L\\) is the Cholesky decomposition of \\(R\\), or \\(LL^T = \\Sigma\\).\nIn our case, we want \\(\\mathbf X\\) to have mean 0 and covariance matrix \\(R\\), so we write\n\\[\n\\mathbf X = L\\mathbf Z\n\\]\nwhere \\(LL^T = R\\).\nWe can skip the manual Cholesky factorization and simply use the {mvtnorm} package.\n\n\nCode\nsample_gaussian_variables &lt;- function(cor_matrix, n_replicates) {\n  mvtnorm::rmvnorm(\n    n = n_replicates,\n    sigma = cor_matrix\n  )\n}\n\nR |&gt; \n  sample_gaussian_variables(n_replicates = 1)\n\n\n         [,1]     [,2]       [,3]       [,4]       [,5]\n[1,] 1.427875 1.837133 -0.1348893 -0.3970323 -0.4613493\n\n\nWe will want more than one observation from each “site”, so we write a helped function for tidying this output into a tibble.\n\n\nCode\ntidy_mvgauss &lt;- function(mvnorm_matrix) {\n  colnames(mvnorm_matrix) &lt;- seq_len(ncol(mvnorm_matrix))\n  \n  mvnorm_matrix |&gt; \n    dplyr::as_tibble() |&gt; \n    dplyr::mutate(\n      replicate = dplyr::row_number(),\n      .before = `1`\n    ) |&gt; \n    tidyr::pivot_longer(\n      c(-replicate),\n      names_to = \"id\", names_transform = as.numeric,\n      values_to = \"Z\"\n    )\n  \n}\n\nn_replicates &lt;- 10\nmvnorm_data &lt;- R |&gt; \n  sample_gaussian_variables(n_replicates = n_replicates) |&gt; \n  tidy_mvgauss()\n\nmvnorm_data |&gt; \n  filter(replicate &lt;= 5) |&gt; \n  pivot_wider(names_from = replicate, values_from = Z) |&gt; \n  gt() |&gt; \n  tab_spanner(\n    columns = -id,\n    label = \"Replicate\"\n  )\n\n\n\n\n\n\n\n\nid\nReplicate\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n-0.4364007\n-0.1443106\n0.7080631\n1.1388723\n1.0330390\n\n\n2\n-0.4017539\n0.6923541\n0.9544816\n-0.4595604\n-1.3986486\n\n\n3\n-0.1161437\n0.9328314\n-0.1228664\n-0.4689164\n-1.1884398\n\n\n4\n1.0500406\n-0.5297770\n-0.8530328\n-1.5227378\n-1.0434809\n\n\n5\n-0.3607882\n-0.5568922\n1.3144552\n0.1006016\n-0.9255078\n\n\n\n\n\n\n\n\n\n3. Transforming to Multivariate GEV\nNow our data, \\(\\mathbf X\\), is multivariate normal with dependence structure according to an AR(1) process, but marginally each \\(X_1\\) is standard normal. To transform this data to a GEV dataset, we simply use the standard normal CDF on each \\(X_i\\) to transform it to \\([0, 1]\\), then we use the GEV quantile function to transform that to a GEV distributed variable.\n\n\nCode\nmvnorm_to_gev &lt;- function(mvnorm_data, gev_params) {\n  \n  mvnorm_data |&gt; \n    dplyr::mutate(\n      U = pnorm(Z)\n    ) |&gt; \n    dplyr::inner_join(\n      gev_params,\n      by = dplyr::join_by(id)\n    ) |&gt; \n    dplyr::mutate(\n      y = purrr:::pmap_dbl(\n        list(U, mu, sigma, xi), \n        \\(U, mu, sigma, xi) evd::qgev(p = U, loc = mu, scale = sigma, shape = xi)\n      )\n    )\n}\n\ngev_params &lt;- expand_grid(\n  mu = 6,\n  sigma = 3,\n  xi = 0.1,\n  id = seq_len(n_id)\n)\n\ngev_data &lt;- mvnorm_data |&gt; \n  mvnorm_to_gev(gev_params = gev_params)\n\ngev_data |&gt; \n  select(-mu, -sigma, -xi) |&gt; \n  gt() |&gt; \n  opt_interactive()"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#checking-the-data",
    "href": "posts/gev_copula_ar1/index.html#checking-the-data",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Checking the data",
    "text": "Checking the data\nLet’s create a function that plots heat maps for each of the three representations of the data\n\nZ: The multivariate normal\nU: The multivariate uniform\ny: The multivariate GEV\n\nWe want the color scales to be free, so we use group_map() and wrap_plots() instead of facet_wrap()\n\n\nCode\nplot_data &lt;- function(data) {\n  data |&gt; \n    select(replicate, id, Z, U, y) |&gt; \n    pivot_longer(c(-replicate, -id)) |&gt; \n    mutate(\n      name = fct_relevel(name, \"Z\", \"U\", \"y\"),\n      name2 = name\n    ) |&gt; \n    group_by(name) |&gt; \n    group_map(\n      function(data, ...) {\n        data |&gt; \n          ggplot(aes(id, replicate, fill = value)) +\n          geom_raster() +\n          scale_fill_viridis_c() +\n          coord_cartesian(expand = FALSE) +\n          theme(legend.position = \"none\") +\n          labs(\n            subtitle = data$name2\n          )\n      }\n    ) |&gt; \n    wrap_plots() +\n    plot_layout(nrow = 1) +\n    plot_annotation(\n      title = \"Heatmaps of the three data representations\",\n      subtitle = \"Z: Normal | U: Uniform | y: GEV\"\n    )\n}\n\n\n\n\nCode\ngev_data |&gt; \n  plot_data()\n\n\n\n\n\n\n\n\n\nIt’s nice to wrap this process in a single function\n\n\nCode\nmake_data &lt;- function(gev_params, n_replicate, rho) {\n  \n  make_AR_cor_matrix_1d(n_id = nrow(gev_params), rho = rho) |&gt; \n    sample_gaussian_variables(n_replicates = n_replicate) |&gt; \n    tidy_mvgauss() |&gt; \n    dplyr::mutate(\n      U = pnorm(Z)\n    ) |&gt; \n    dplyr::inner_join(\n      gev_params,\n      by = dplyr::join_by(id)\n    ) |&gt; \n    dplyr::mutate(\n      y = purrr::pmap_dbl(\n        list(U, mu, sigma, xi), \n        \\(U, mu, sigma, xi) evd::qgev(p = U, loc = mu, scale = sigma, shape = xi)\n      )\n    )\n}\n\n\nLet’s see how the data look if we make it larger and increase the correlation\n\n\nCode\nn_id &lt;- 40\nn_replicates &lt;- 100\n\ngev_params &lt;- expand_grid(\n  mu = 6,\n  sigma = 3,\n  xi = 0.1,\n  id = seq_len(n_id)\n)\n\nd &lt;- make_data(gev_params, n_replicates, rho = 0.95)\n\n\nWe see that with higher neighbor correlations we get obvious horizontal stripes corresponding to a large amount of dependence within each replicate.\n\n\nCode\nd |&gt; plot_data()"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#modeling",
    "href": "posts/gev_copula_ar1/index.html#modeling",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Modeling",
    "text": "Modeling\nR scripts used for modeling\n\nAR(1) Model\nWe can fit the AR(1) model directly using Stan.\n\n\nCode\nfunctions {\n  real normal_ar1_lpdf(vector x, real rho) {\n    int N = num_elements(x);\n    real out;\n    real log_det = - (N - 1) * (log(1 + rho) + log(1 - rho)) / 2;\n    vector[N] q;\n    real scl = sqrt(1 / (1 - rho^2));\n    \n    q[1:(N - 1)] = scl * (x[1:(N - 1)] - rho * x[2:N]);\n    q[N] = x[N];\n    \n    out = log_det - dot_self(q) / 2;\n    \n    return out;\n  }\n\n  real normal_copula_ar1_lpdf(vector U, real rho) {\n    int N = rows(U);\n    vector[N] Z = inv_Phi(U);\n    return normal_ar1_lpdf(Z | rho) + dot_self(Z) / 2;\n  }\n\n  real gev_cdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return exp(-exp(z));\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return exp(-pow(z, -1/xi));\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  } \n\n  real gev_lpdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return -log(sigma) - z - exp(-z);\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return -log(sigma) - (1 + 1/xi) * log(z) - pow(z, -1/xi);\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n}\n\ndata {\n  int&lt;lower = 0&gt; n_replicate;\n  int&lt;lower = 0&gt; n_id;\n  array[n_replicate, n_id] real y;\n  array[n_replicate, n_id] real y_test;\n}\n\nparameters {\n  real&lt;lower = 0&gt; mu;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = -0.5, upper = 1&gt; xi;\n  real&lt;lower = -1, upper = 1&gt; rho;\n}\n\nmodel {\n  for (i in 1:n_replicate) {\n    vector[n_id] U;\n    for (j in 1:n_id) {    \n      U[j] = gev_cdf(y[i, j] | mu, sigma, xi);\n      target += gev_lpdf(y[i, j] | mu, sigma, xi);\n    } \n    target += normal_copula_ar1_lpdf(U | rho);\n  }\n}\n \ngenerated quantities {\n  real log_lik = 0;\n  {\n    \n    for (i in 1:n_replicate) {\n      vector[n_id] U;\n      for (j in 1:n_id) {\n        U[j] = gev_cdf(y_test[i, j] | mu, sigma, xi);\n        log_lik += gev_lpdf(y_test[i, j] | mu, sigma, xi);\n      }\n      log_lik += normal_copula_ar1_lpdf(U | rho);\n    }\n  }\n  \n}\n\n\n\n\ni.i.d. Model\n\n\nCode\nfunctions {\n  real gev_lpdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return -log(sigma) - z - exp(-z);\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return -log(sigma) - (1 + 1/xi) * log(z) - pow(z, -1/xi);\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n}\n\ndata {\n  int&lt;lower = 0&gt; n_replicate;\n  int&lt;lower = 0&gt; n_id;\n  array[n_replicate, n_id] real y;\n  array[n_replicate, n_id] real y_test;\n}\n\nparameters {\n  real&lt;lower = 0&gt; mu;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = -0.5, upper = 1&gt; xi;\n}\n\ntransformed parameters {\n  \n}\n\nmodel {\n  for (i in 1:n_replicate) {\n    for (j in 1:n_id) {\n      target += gev_lpdf(y[i, j] | mu, sigma, xi);\n    }\n  }\n}\n\ngenerated quantities {\n  real log_lik = 0;\n  {\n    \n    for (i in 1:n_replicate) {\n      for (j in 1:n_id) {\n        log_lik += gev_lpdf(y_test[i, j] | mu, sigma, xi);\n      }\n    }\n  }\n}\n\n\n\n\nTwo-Step Estimation\nWe can’t fit this model directly using one stan file. What we do is:\n\nEstimate the marginal distributions\nUse the empirical CDFs to transform the data into uniformly distributed variables\nTransform these uniformly distributed variables into standard normal variables\nEstimate the precision matrix using these variables\nMake sure the precision matrix is semi-positive definite and normalize it so that its inverse is a correlation matrix\nRe-estimate the marginal distributions with a gaussian copula assuming this precision matrix is known\n\n\n\nCode\nfunctions {\n  real gev_cdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return exp(-exp(z));\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return exp(-pow(z, -1/xi));\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n\n  real gev_lpdf(real y, real mu, real sigma, real xi) {\n    if (abs(xi) &lt; 1e-10) {\n      real z = (y - mu) / sigma;\n      return -log(sigma) - z - exp(-z);\n    } else {\n      real z = 1 + xi * (y - mu) / sigma;\n      if (z &gt; 0) {\n        return -log(sigma) - (1 + 1/xi) * log(z) - pow(z, -1/xi);\n      } else {\n        reject(\"Found incompatible GEV parameter values\");\n      }\n    }\n  }\n\n  real normal_prec_chol_lpdf(vector x, array[] int n_values, array[] int index, vector values, real log_det) {\n    int N = num_elements(x);\n    int counter = 1;\n    vector[N] q = rep_vector(0, N);\n\n    for (i in 1:N) {\n      for (j in 1:n_values[i]) {\n        q[i] += values[counter] * x[index[counter]];\n        counter += 1;\n      }\n    }\n\n    return log_det - dot_self(q) / 2;\n\n  }\n\n  real normal_copula_prec_chol_lpdf(vector U, array[] int n_values, array[] int index, vector value, real log_det) {\n    int N = rows(U);\n    vector[N] Z = inv_Phi(U);\n\n    return normal_prec_chol_lpdf(Z | n_values, index, value, log_det) + dot_self(Z) / 2;\n  }\n\n}\n\ndata {\n  int&lt;lower = 0&gt; n_replicate;\n  int&lt;lower = 0&gt; n_id;\n  array[n_replicate, n_id] real y;\n  array[n_replicate, n_id] real y_test;\n\n  int&lt;lower = 0&gt; n_nonzero_chol_Q;\n  real log_det_Q;\n  array[n_id] int n_values;\n  array[n_nonzero_chol_Q] int index;\n  vector[n_nonzero_chol_Q] value;\n}\n\nparameters {\n  real&lt;lower = 0&gt; mu;\n  real&lt;lower = 0&gt; sigma;\n  real&lt;lower = -0.5, upper = 1&gt; xi;\n}\n\nmodel {\n  for (i in 1:n_replicate) {\n    for (j in 1:n_id) {\n      target += gev_lpdf(y[i, j] | mu, sigma, xi);\n    }\n  }\n}\n \ngenerated quantities {\n  real log_lik = 0;\n  {\n    \n    for (i in 1:n_replicate) {\n      vector[n_id] U;\n      for (j in 1:n_id) {\n        U[j] = gev_cdf(y_test[i, j] | mu, sigma, xi);\n        log_lik += gev_lpdf(y_test[i, j] | mu, sigma, xi);\n      }\n      log_lik += normal_copula_prec_chol_lpdf(U | n_values, index, value, log_det_Q);\n    }\n  }\n  \n}"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#estimates-of-gev-parameters",
    "href": "posts/gev_copula_ar1/index.html#estimates-of-gev-parameters",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Estimates of GEV Parameters",
    "text": "Estimates of GEV Parameters"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#expected-log-predicted-probability-density",
    "href": "posts/gev_copula_ar1/index.html#expected-log-predicted-probability-density",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Expected log predicted probability density",
    "text": "Expected log predicted probability density\n\n\nComparing to the simple i.i.d. model"
  },
  {
    "objectID": "posts/gev_copula_ar1/index.html#differences-between-ar1-and-two-step-model",
    "href": "posts/gev_copula_ar1/index.html#differences-between-ar1-and-two-step-model",
    "title": "Applying a Gaussian AR(1) Copula to Generalized Extreme Value Margins",
    "section": "Differences between AR(1) and Two-Step Model",
    "text": "Differences between AR(1) and Two-Step Model"
  },
  {
    "objectID": "posts/ceda-archive/index.html",
    "href": "posts/ceda-archive/index.html",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(bggjphd)\ntheme_set(theme_bggj())"
  },
  {
    "objectID": "posts/ceda-archive/index.html#setup",
    "href": "posts/ceda-archive/index.html#setup",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "Setup",
    "text": "Setup\nThe first things you’re going to need are the following:\n\nA CEDA Archive account\nA CEDA FTP password\nThe location of the files you want to download.\nOn your dataset page, press download and navigate to the subset you want to fetch"
  },
  {
    "objectID": "posts/ceda-archive/index.html#file-location",
    "href": "posts/ceda-archive/index.html#file-location",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "File Location",
    "text": "File Location\nIn my case the 720 files are located at\n\n\nCode\nurl &lt;- \"ftp://ftp.ceda.ac.uk/badc/ukcp18/data/land-cpm/uk/5km/rcp85/01/pr/1hr/v20210615/\""
  },
  {
    "objectID": "posts/ceda-archive/index.html#authentication",
    "href": "posts/ceda-archive/index.html#authentication",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "Authentication",
    "text": "Authentication\nWe’re going to need to input our username and password into the URL to download the data. In order to hide my login info when coding I put it in my R Environment (easy to edit with usethis::edit_r_environ()) and can thus write a function to input it in requests. I never assign my info to variables, but rather just use functions to input them.\n\n\nCode\nuserpwd &lt;- function() {\n  str_c(\n    Sys.getenv(\"CEDA_USR\"), \n    Sys.getenv(\"CEDA_PWD\"), \n    sep = \":\"\n  )\n}\n\n\nNow we can send a request to the FTP server in order to get a list of all the files we want to download\n\n\nCode\nfilenames &lt;- RCurl::getURL(\n  url,\n  userpwd = userpwd(),\n  dirlistonly = TRUE\n)\n\n\nAs you can see below, the result is given to us as one long string.\n\n\nCode\nstringr::str_sub(\n  filenames,\n  start = 1,\n  end = 160\n)\n\n\n[1] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19801201-19801230.nc\\npr_rcp85_land-cpm_uk_5km_01_1hr_19810101-19810130.nc\\npr_rcp85_land-cpm_uk_5km_01_1hr_19810201-19810230.nc\\np\""
  },
  {
    "objectID": "posts/ceda-archive/index.html#cleaning-up-the-file-names",
    "href": "posts/ceda-archive/index.html#cleaning-up-the-file-names",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "Cleaning up the file names",
    "text": "Cleaning up the file names\nWe get a single string with all the file names. It’s easy to split them up into separate strings ands remove the trailing empty line.\n\n\nCode\nfiles &lt;- filenames |&gt;\n  stringr::str_split_1(pattern = \"\\n\")\n\nfiles &lt;- files[-length(files)]\n\nhead(files)\n\n\n[1] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19801201-19801230.nc\"\n[2] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19810101-19810130.nc\"\n[3] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19810201-19810230.nc\"\n[4] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19810301-19810330.nc\"\n[5] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19810401-19810430.nc\"\n[6] \"pr_rcp85_land-cpm_uk_5km_01_1hr_19810501-19810530.nc\""
  },
  {
    "objectID": "posts/ceda-archive/index.html#writing-data-processing-functions",
    "href": "posts/ceda-archive/index.html#writing-data-processing-functions",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "Writing data processing functions",
    "text": "Writing data processing functions\nNow comes the tricky part. We are going to download 720 files (one for each month) that are around 120MB each. If we just download them and keep them on our hard drive that’s going to be upwards of 70GB. Instead of doing that we will use the function process_data() below to do the following:\n\nFor each dataset\n\n\nCreate a temporary file\nDownload the data into the temporary file\nFor each location, throw away all measurements except for the maximum\nCreate a tidy table with information about the coordinates of the location, the max precipitation and the observation date-range\nDelete the temporary file\n\nBefore we can iterate we will need to create a new helper function. Since we will now be using download.file() to download our data sets, we need to input our username and password into the URL. As before, in order to not reveal our information we use functions instead of creating global variables in the environment. Thus we won’t accidentally leak our information when for example taking screenshots.\n\n\nCode\nmake_download_path &lt;- function(filename) {\n  url |&gt;\n    stringr::str_replace(\"//\", stringr::str_c(\"//\", userpwd(), \"@\")) |&gt;\n    stringr::str_c(filename)\n}\n\n\nThe data files are stored in .nc form. The ncdf4 package lets us connect to these kinds of files and pull in the variables we need.\n\n\nCode\nprocess_data &lt;- function(filename) {\n  \n  Sys.sleep(0.1)\n  \n  from_to &lt;- stringr::str_extract_all(filename, \"_[0-9]{8}-[0-9]{8}\")[[1]] |&gt;\n    stringr::str_replace(\"_\", \"\") |&gt;\n    stringr::str_split_1(\"-\")\n  \n  from &lt;- as.Date(from_to[1], format = \"%Y%m%d\")\n  to &lt;- from + lubridate::months(1, abbreviate = FALSE) - lubridate::days(1)\n  \n  tmp &lt;- tempfile()\n  \n  download.file(\n    make_download_path(filename),\n    tmp,\n    mode = \"wb\",\n    quiet = TRUE\n  )\n  \n  temp_d &lt;- ncdf4::nc_open(tmp)\n  \n  max_pr &lt;- ncdf4::ncvar_get(temp_d, \"pr\") |&gt;\n    apply(MARGIN = c(1, 2), FUN = max)\n  \n  lat &lt;- ncdf4::ncvar_get(temp_d, \"latitude\")\n  long &lt;- ncdf4::ncvar_get(temp_d, \"longitude\")\n  \n  out &lt;- tidyr::crossing(\n    proj_x = 1:180,\n    proj_y = 1:244,\n    from_date = from,\n    to_date = to\n  ) |&gt;\n    dplyr::arrange(proj_y, proj_x) |&gt;\n    dplyr::mutate(\n      precip = as.numeric(max_pr),\n      longitude = as.numeric(long),\n      latitude = as.numeric(lat),\n      station = row_number()\n    )\n  \n  out\n}"
  },
  {
    "objectID": "posts/ceda-archive/index.html#putting-it-all-together",
    "href": "posts/ceda-archive/index.html#putting-it-all-together",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "Putting it all together",
    "text": "Putting it all together\nHaving defined our function we throw it into purrr::map_dfr() (map_dfr() tells R that the output should be a dataframe in which the iteration results are concatenated rowwise) for iteration and say yes please to a progress bar. I could have used the furrr package to reduce the time by downloading multiple files in parallel, but I was afraid of getting timed out from the CEDA FTP server so I decided to just be patient.\n\n\nCode\nd &lt;- files |&gt;\n  purrr::map_dfr(process_data, .progress = TRUE)\n\n\nHaving created our dataset we write it out to disk using everyone’s favorite new format parquet. This way we can efficiently query the data without reading it into memory using arrow::open_dataset().\nThis whole process took 3 hours and 21 minutes on my computer. The largest bottleneck by far was downloading the data.\n\n\nCode\nd |&gt;\n  arrow::write_parquet(\"monthly_data.parquet\")"
  },
  {
    "objectID": "posts/ceda-archive/index.html#final-processing",
    "href": "posts/ceda-archive/index.html#final-processing",
    "title": "Fetching FTP data from the Ceda Archives",
    "section": "Final processing",
    "text": "Final processing\nI mentioned above that I only needed the yearly data, but currently the dataset contains monthly maxima. Since I might need to do seasonal modeling later in my PhD I decided it would be smart to keep the monthly data, but it’s also very easy to further summarise the data into yearly maxima.\nSince the data for 1980 contain only one month, I decided to not include that year as it is not really a true yearly maximum.\n\n\nCode\nd &lt;- d |&gt;\n  dplyr::mutate(year = lubridate::year(from_date)) |&gt;\n  dplyr::filter(year &gt; 1980) |&gt;\n  dplyr::group_by(year, station, proj_x, proj_y, longitude, latitude) |&gt;\n  dplyr::summarise(\n    precip = max(precip),\n    .groups = \"drop\"\n  )\n\nd |&gt;\n  arrow::write_parquet(\"yearly_data.parquet\")"
  },
  {
    "objectID": "posts/t-copula/index.html",
    "href": "posts/t-copula/index.html",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(bayesplot)\nlibrary(patchwork)\nlibrary(gt)\ntheme_set(bggjphd::theme_bggj())\nThis post continues our series on copulas in Stan by introducing the t-copula and comparing it with the Gaussian copula we covered previously. We’ll focus particularly on how the t-copula’s tail dependence makes it more suitable for modeling extreme events.\nOther posts in this series:"
  },
  {
    "objectID": "posts/t-copula/index.html#what-is-the-t-copula",
    "href": "posts/t-copula/index.html#what-is-the-t-copula",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "What is the t-Copula?",
    "text": "What is the t-Copula?\nThe t-copula is derived from the multivariate t-distribution, just as the Gaussian copula comes from the multivariate normal. It has two key parameters:\n\nA correlation matrix \\(\\Sigma\\) (like the Gaussian copula)\nDegrees of freedom \\(\\nu\\) (unique to the t-copula)\n\nThe degrees of freedom parameter \\(\\nu\\) controls the heaviness of the tails:\n\nLower values \\(\\to\\) heavier tails \\(\\to\\) stronger tail dependence\nHigher values \\(\\to\\) lighter tails \\(\\to\\) approaches the Gaussian copula\nAs \\(\\nu \\to \\infty\\), the t-copula becomes the Gaussian copula"
  },
  {
    "objectID": "posts/t-copula/index.html#building-intuition-the-random-scale-factor",
    "href": "posts/t-copula/index.html#building-intuition-the-random-scale-factor",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Building Intuition: The Random Scale Factor",
    "text": "Building Intuition: The Random Scale Factor\nOne of the most intuitive ways to understand the t-copula is through its stochastic representation:\n\nStart with correlated normal variables \\(\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)\\)\nGenerate a random scale factor \\(W \\sim \\chi^2_\\nu\\)\nMultiply everything by \\(\\sqrt{\\nu/W}\\) to get \\(\\mathbf{X} = \\sqrt{\\nu/W} \\cdot \\mathbf{Z}\\)\n\nThis shared scaling creates tail dependence:\n\nWhen \\(W\\) is small, all variables become large together\nThe smaller \\(\\nu\\) is, the more variable \\(W\\) becomes\nThis creates more frequent joint extreme events than the Gaussian copula"
  },
  {
    "objectID": "posts/t-copula/index.html#understanding-tail-dependence",
    "href": "posts/t-copula/index.html#understanding-tail-dependence",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Understanding Tail Dependence",
    "text": "Understanding Tail Dependence\nBefore we proceed further, let’s formally define tail dependence. For a bivariate copula C, the upper and lower tail dependence coefficients are defined as:\n\\[\n\\begin{aligned}\n\\lambda_U &= \\lim_{u \\to 1} P(U_2 &gt; F_2^{-1}(u) | U_1 &gt; F_1^{-1}(u)) \\\\\n&= \\lim_{u \\to 1} \\frac{1-2u+C(u,u)}{1-u} \\\\\n\\lambda_L &= \\lim_{u \\to 0} P(U_2 \\leq F_2^{-1}(u) | U_1 \\leq F_1^{-1}(u)) \\\\\n&= \\lim_{u \\to 0} \\frac{C(u,u)}{u}\n\\end{aligned}\n\\]\nIntuitively, \\(\\lambda_U\\) measures the probability that one variable is extremely large given that another is extremely large, while \\(\\lambda_L\\) measures the probability that one variable is extremely small given that another is extremely small.\nDue to radial symmetry, the t-copula has identical lower- and upper-tail dependence, given by:\n\\[\n\\lambda_U = \\lambda_L = 2t_{\\nu+1}\\left(-\\sqrt{\\frac{(\\nu + 1)(1-\\rho)}{1+\\rho}}\\right)\n\\]\nwhere:\n\n\\(t_{\\nu+1}\\) is the cumulative distribution function of a univariate t-distribution with \\(\\nu+1\\) degrees of freedom\n\\(\\rho\\) is the correlation parameter\n\\(\\nu\\) is the degrees of freedom parameter\n\nSome key properties of this tail dependence:\n\nIt is decreasing in \\(\\nu\\) for fixed \\(\\rho\\) (more degrees of freedom \\(\\to\\) less tail dependence)\nIt is increasing in \\(\\rho\\) for fixed \\(\\nu\\) (stronger correlation \\(\\to\\) stronger tail dependence)\nAs \\(\\nu \\to \\infty\\), \\(\\lambda_U = \\lambda_L \\to 0\\) for \\(|\\rho| &lt; 1\\), recovering the Gaussian case\nFor \\(\\rho = 1\\), we have \\(\\lambda_U = \\lambda_L = 1\\) regardless of \\(\\nu\\)\nEven for \\(\\rho = 0\\), we have non-zero tail dependence for finite \\(\\nu\\).\n\n\nCode\nt_tail_dependence &lt;- function(rho, nu) {\n  2 * pt(-sqrt((nu + 1) * (1 - rho) / (1 + rho)), nu + 1)\n}\n\ncrossing(\n  nu = c(2, 4, 8, 16, 32, 64, 128),\n  rho = seq(-1, 1, length.out = 400)\n) |&gt;\n  mutate(\n    lambda = t_tail_dependence(rho, nu)\n  ) |&gt;\n  ggplot(aes(rho, lambda, color = as.factor(nu))) +\n  geom_line(\n    linewidth = 1.5\n  ) +\n  scale_x_continuous(\n    expand = c(0, 0),\n    limits = c(-1, 1)\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0),\n    limits = c(0, 1)\n  ) +\n  scale_colour_brewer(\n    palette = \"Blues\",\n    direction = -1\n  ) +\n  labs(\n    x = expression(rho),\n    y = expression(lambda[U] == lambda[L]),\n    color = expression(nu),\n    title = \"Asymptotic tail dependence of the t-copula\"\n  )\ncrossing(\n  nu = seq(1, 32, length.out = 100),\n  rho = 0\n) |&gt;\n  mutate(\n    lambda = t_tail_dependence(rho, nu)\n  ) |&gt;\n  ggplot(aes(nu, lambda)) +\n  geom_line(linewidth = 1.5) +\n  scale_x_continuous(\n    expand = c(0, 0),\n    limits = c(0, 32)\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0),\n    limits = c(NA, 0.35),\n    trans = \"log10\",\n    labels = scales::label_log()\n  ) +\n  labs(\n    x = expression(nu),\n    y = expression(lambda[U] == lambda[L]),\n    subtitle = \"Even for zero correlation, the t-copula has non-zero tail dependence\"\n  )"
  },
  {
    "objectID": "posts/t-copula/index.html#tail-dependence",
    "href": "posts/t-copula/index.html#tail-dependence",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Tail Dependence",
    "text": "Tail Dependence\nUnlike the Gaussian copula, the t-copula exhibits tail dependence. The upper and lower tail dependence coefficients are:\n\\[\n\\lambda_U = \\lambda_L = 2t_{\\nu+1}\\left(-\\sqrt{\\frac{(\\nu + 1)(1-\\rho)}{1+\\rho}}\\right)\n\\]\nThis symmetry (\\(\\lambda_U = \\lambda_L\\)) means the t-copula treats both tails equally."
  },
  {
    "objectID": "posts/t-copula/index.html#limiting-cases",
    "href": "posts/t-copula/index.html#limiting-cases",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Limiting Cases",
    "text": "Limiting Cases\n\nAs \\(\\nu \\to \\infty\\): Converges to Gaussian copula\nAs \\(\\nu \\to 0\\): Tail dependence increases1\n\\(\\nu = 1\\): Cauchy copula (strongest tail dependence)\n\n1 Lower values of \\(\\nu\\) produce heavier tails and increase tail dependence. However, taking \\(\\nu\\) extremely close to zero is more of a theoretical construct than a practical modeling choice. In most applications, \\(\\nu\\) is chosen within a range that provides heavier tails than the Gaussian case, but not so extreme as to be unrealistic."
  },
  {
    "objectID": "posts/t-copula/index.html#concordance-measures",
    "href": "posts/t-copula/index.html#concordance-measures",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Concordance Measures",
    "text": "Concordance Measures\nLike the Gaussian copula:\n\nKendall’s \\(\\tau\\): \\(\\tau = \\frac{2}{\\pi}\\arcsin(\\rho)\\)\nSpearman’s \\(\\rho\\): \\(\\rho_S = \\frac{6}{\\pi}\\arcsin(\\rho/2)\\)\n\nwhere \\(\\rho\\) is the correlation coefficient.2\n2 Note that these concordance measures are identical to the Gaussian copula because multiplying by the scaling factor \\(\\sqrt{\\nu/W}\\) affects the magnitude of the variables but preserves their ranks. Since Kendall’s \\(\\tau\\) and Spearman’s \\(\\rho\\) only depend on the ranks of the data, not their actual values, they remain unchanged from the Gaussian case."
  },
  {
    "objectID": "posts/t-copula/index.html#sampling-the-data",
    "href": "posts/t-copula/index.html#sampling-the-data",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Sampling the Data",
    "text": "Sampling the Data\n\nIn words\nTo sample from this data-generating process we\n\nGenerate \\(\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_D)\\)\nGenerate \\(W \\sim \\text{Inv-}\\chi^2(\\nu)\\) independently\nApply correlation structure: \\(\\mathbf{Z}' = \\mathbf{L}\\mathbf{Z}\\)\nScale variables: \\(\\mathbf{X} = \\sqrt{\\nu/W}\\cdot\\mathbf{Z}'\\)\nTransform to uniform: \\(\\mathbf{U} = t_\\nu(\\mathbf{X})\\)\nTransform to exponential: \\(\\mathbf{Y} = F^{-1}_{\\text{Exp}}(\\mathbf{U}|\\boldsymbol{\\lambda})\\)\n\n\n\nIn code\n\n\nCode\nn_obs &lt;- 50\nrho12 &lt;- 0.8\nrho13 &lt;- 0.1\nrho23 &lt;- 0.4\nlambda1 &lt;- 2\nlambda2 &lt;- 4\nlambda3 &lt;- 6\ndf &lt;- 4\nsigma &lt;- matrix(\n  c(\n    1, rho12, rho13,\n    rho12, 1, rho23,\n    rho13, rho23, 1\n  ),\n  nrow = 3\n)\nL &lt;- chol(sigma)\n\nset.seed(1)\nW &lt;- rchisq(n_obs, df = df)\nZ &lt;- matrix(rnorm(n = n_obs * 3), nrow = 3)\nZ &lt;- sqrt(df / W) * t(L %*% Z)\n\nd &lt;- tibble(\n  z1 = Z[, 1],\n  z2 = Z[, 2],\n  z3 = Z[, 3],\n  time = seq_len(n_obs)\n)  |&gt; \n  pivot_longer(\n    c(-time), \n    names_to = \"variable\", \n    names_transform = parse_number,\n    values_to = \"z\"\n  ) |&gt; \n  inner_join(\n    tibble(\n      variable = c(1, 2, 3),\n      lambda = c(lambda1, lambda2, lambda3)\n    )\n  ) |&gt; \n  mutate(\n    u = pt(z, df = df),\n    y = qexp(u, rate = lambda)\n  )\n\n\n\n\nCode\nd |&gt;\n  select(-lambda) |&gt;\n  pivot_longer(c(z, u, y)) |&gt;\n  pivot_wider(names_from = variable, names_prefix = \"v\") |&gt;\n  mutate(\n    name = fct_relevel(name, \"z\", \"u\") |&gt;\n      fct_recode(\n        \"Student-t\" = \"z\",\n        \"Uniform\" = \"u\",\n        \"Exponential\" = \"y\"\n      )\n  ) |&gt;\n  group_by(n2 = name) |&gt;\n  group_map(\n    \\(data, ...) {\n      # X2 vs X1\n      p12 &lt;- data |&gt;\n        ggplot(aes(v1, v2)) +\n        geom_density_2d_filled(alpha = 0.5) +\n        geom_point(size = 1.4) +\n        scale_x_continuous(\n          expand = c(0, 0)\n        ) +\n        scale_y_continuous(\n          expand = c(0, 0)\n        ) +\n        theme(legend.position = \"none\") +\n        labs(\n          subtitle = unique(data$name),\n          x = expression(X[1]),\n          y = expression(X[2])\n        )\n\n      # X3 vs X1\n      p13 &lt;- data |&gt;\n        ggplot(aes(v1, v3)) +\n        geom_density_2d_filled(alpha = 0.5) +\n        geom_point(size = 1.4) +\n        scale_x_continuous(\n          expand = c(0, 0)\n        ) +\n        scale_y_continuous(\n          expand = c(0, 0)\n        ) +\n        theme(legend.position = \"none\") +\n        labs(\n          subtitle = unique(data$name),\n          x = expression(X[1]),\n          y = expression(X[3])\n        )\n\n      # X3 vs X2\n      p23 &lt;- data |&gt;\n        ggplot(aes(v2, v3)) +\n        geom_density_2d_filled(alpha = 0.5) +\n        geom_point(size = 1.4) +\n        scale_x_continuous(\n          expand = c(0, 0)\n        ) +\n        scale_y_continuous(\n          expand = c(0, 0)\n        ) +\n        theme(legend.position = \"none\") +\n        labs(\n          subtitle = unique(data$name),\n          x = expression(X[2]),\n          y = expression(X[3])\n        )\n\n      wrap_plots(p12, p13, p23, nrow = 3)\n    }\n  ) |&gt;\n  wrap_plots(\n    ncol = 3,\n    widths = c(1, 1,1 )\n  ) +\n  plot_annotation(\n    title = \"Going from Student-t to Uniform to Exponential\"\n  )"
  },
  {
    "objectID": "posts/t-copula/index.html#stan-model",
    "href": "posts/t-copula/index.html#stan-model",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Stan Model",
    "text": "Stan Model\nThe Stan implementation has three main components:\n\nStudent-t Copula Log-Density\nSimilar to the Gaussian copula, we need to implement the t-copula log density:\n\n\nCode\nreal t_copula_lpdf(vector u, matrix L, real nu) {\n  int D = num_elements(u);\n  vector[D] x;\n  real logp;\n\n  // Transform U to X via the inverse t CDF\n  for (d in 1:D) {\n    x[d] = student_t_icdf(u[d], nu);\n  }\n\n  // Multivariate t density minus sum of univariate t densities\n  logp = multi_student_t_cholesky_lpdf(x | nu, rep_vector(0, D), L);\n  for (d in 1:D) {\n    logp -= student_t_lpdf(x[d] | nu, 0, 1);\n  }\n\n  return logp;\n}\n\n\nThe key differences from the Gaussian copula are:\n\nWe use the t-distribution’s quantile function instead of the normal quantile function\nWe use the multivariate t-distribution instead of the multivariate normal\nWe have an additional parameter nu for the degrees of freedom\n\n\n\nStudent-t Quantile Function:\nUnlike the Gaussian copula where we could use Stan’s built-in inv_Phi(), we need to implement the t-distribution’s quantile function ourselves. The implementation in the Stan model follows numerical approximations for different ranges of the input values to ensure stability and accuracy.\nThe function is too long to paste here, but it is based on Sean Pinkey’s implementation from the Stan forums\n\n\nModel Specification\nThe model follows a similar structure to the Gaussian copula but with the addition of the degrees of freedom parameter:\n\n\nCode\nparameters {\n  vector&lt;lower=0&gt;[D] lambda;\n  cholesky_factor_corr[D] L;\n  real&lt;lower=1&gt; nu;   // degrees of freedom for the t-copula\n}\n\nmodel {\n  matrix[N, D] U;\n  for (i in 1:N) {\n    // Transform data to uniforms using exponential CDF\n    for (j in 1:D) {\n      target += exponential_lpdf(Y[i, j] | lambda[j]);\n      U[i, j] = exponential_cdf(Y[i, j] | lambda[j]);\n    }\n    // Add the t-copula contribution\n    target += t_copula_lpdf(to_vector(U[i, ]) | L, nu);\n  }\n\n  // Priors\n  target += lkj_corr_cholesky_lpdf(L | 1.0);\n  nu ~ gamma(2, 0.1);  // Prior for degrees of freedom\n  lambda ~ exponential(1);\n}\n\n\n\n\nGenerated Quantities\nFor posterior predictive checks, we need to:\n\nGenerate samples from the multivariate t-distribution\nTransform them to uniform variables using the t CDF\nTransform to exponential variables using the exponential quantile function\n\n\n\nCode\ngenerated quantities {\n  corr_matrix[D] Sigma = multiply_lower_tri_self_transpose(L);\n  matrix[N, D] yrep;\n\n  {\n    matrix[N, D] U_rep;\n    matrix[N, D] Z_rep;\n\n    for (i in 1:N) {\n      Z_rep[i] = (multi_student_t_cholesky_rng(nu, rep_vector(0, D), L))';\n      for (j in 1:D) {\n        U_rep[i, j] = student_t_cdf(Z_rep[i, j] | nu, 0, 1);\n        yrep[i, j] = exponential_icdf(U_rep[i, j], lambda[j]);\n      }\n    }\n  }\n}\n\n\nThe main differences from the Gaussian copula’s generated quantities block are:\n\nWe use multi_student_t_cholesky_rng() instead of multi_normal_cholesky_rng()\nWe use student_t_cdf() instead of Phi()\nThe degrees of freedom parameter nu is passed to both functions\nThis implementation allows us to model tail dependence that the Gaussian copula cannot capture, while maintaining the same marginal distributions."
  },
  {
    "objectID": "posts/t-copula/index.html#sampling-from-the-posterior",
    "href": "posts/t-copula/index.html#sampling-from-the-posterior",
    "title": "It was the best of tails, it was the worst of tails: The T-Copula",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\nPrepare the data and sample from the model.\n\n\nCode\nY &lt;- d |&gt;\n  select(time, variable, y) |&gt; \n  pivot_wider(names_from = variable, values_from = y) |&gt; \n  select(-time) |&gt; \n  as.matrix()\n\nstan_data &lt;- list(\n  Y = Y,\n  N = nrow(Y),\n  D = ncol(Y)\n)\n\nexample1 &lt;- cmdstan_model(here::here(\"posts\", \"t-copula\", \"Stan\", \"t-copula.stan\"))\n\nresult &lt;- example1$sample(\n  data = stan_data,\n  chains = 4,\n  seed = 1,\n  parallel_chains = 4,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\n\n\n\nCode\nresult$summary(c(\"lambda\", \"Sigma[1,2]\", \"Sigma[1,3]\", \"Sigma[2,3]\", \"nu\")) |&gt; \n  gt() |&gt; \n  fmt_number()\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlambda[1]\n1.53\n1.52\n0.20\n0.19\n1.22\n1.86\n1.00\n3,394.27\n2,825.60\n\n\nlambda[2]\n3.57\n3.55\n0.50\n0.51\n2.79\n4.45\n1.00\n2,607.47\n2,528.29\n\n\nlambda[3]\n6.08\n6.05\n0.79\n0.78\n4.85\n7.44\n1.00\n2,922.44\n2,475.93\n\n\nSigma[1,2]\n0.43\n0.44\n0.12\n0.12\n0.21\n0.61\n1.00\n2,873.68\n2,756.21\n\n\nSigma[1,3]\n0.19\n0.20\n0.13\n0.14\n−0.04\n0.41\n1.00\n2,875.84\n2,802.58\n\n\nSigma[2,3]\n0.74\n0.75\n0.08\n0.07\n0.60\n0.84\n1.00\n3,029.44\n2,799.22\n\n\nnu\n3.55\n3.29\n1.23\n1.07\n2.00\n5.88\n1.00\n3,499.92\n2,999.41\n\n\n\n\n\n\n\n\n\nCode\nmcmc_trace(\n  result$draws(), \n  pars = c(\"lambda[1]\", \"lambda[2]\", \"Sigma[1,2]\", \"nu\")\n  )\n\n\n\n\n\n\n\n\n\n\nCode\nyrep &lt;- result$draws(\"yrep\", format = \"matrix\")\ny &lt;- as.numeric(Y)\n\nppc_dens_overlay(\n  y = y[seq_len(n_obs)], \n  yrep = yrep[1:100, seq_len(n_obs)]\n) + \n  ggtitle(expression(X[1]))\nppc_dens_overlay(\n  y = y[n_obs + seq_len(n_obs)], \n  yrep = yrep[1:100, n_obs + seq_len(n_obs)]\n) +\n  ggtitle(expression(X[2]))\nppc_dens_overlay(\n  y = y[2 * n_obs + seq_len(n_obs)], \n  yrep = yrep[1:100, 2 * n_obs + seq_len(n_obs)]\n) +\n  ggtitle(expression(X[2]))"
  },
  {
    "objectID": "posts/stan-copulas-1/index.html",
    "href": "posts/stan-copulas-1/index.html",
    "title": "If It Bleeds, We Can Kill It",
    "section": "",
    "text": "Welcome to the first post in my series on copulas in Stan. After StanCon 2024 I was inspired to start writing short blog posts about this both to help get other started and also because I often don’t really know how a thing works until I have to write about it or present it.\nIf you’ve ever felt intimidated by Sklar’s theorem or how the Frank Copula is defined\n\\[\nC_\\theta^F(\\mathbf u) = -\\frac1\\theta\\log\\left(1 + \\frac{(e^{-\\theta u_1} - 1)(e^{-\\theta u_2} - 1)}{e^{-\\theta} - 1}\\right),\n\\]\njust remember Arnold’s famous words from Predator"
  },
  {
    "objectID": "posts/stan-copulas-1/index.html#copulas-as-densities",
    "href": "posts/stan-copulas-1/index.html#copulas-as-densities",
    "title": "If It Bleeds, We Can Kill It",
    "section": "Copulas as Densities",
    "text": "Copulas as Densities\nCopulas are multivariate distribution functions for random variables with uniform marginal distributions, i.e. they are functions that map the unit cube \\([0,1]^D\\) to \\([0,1]\\). They can also be described using copula density functions when the marginals are continuous. If \\(H(X)\\) is the CDF of \\(X\\), and the multivariate distribution has a PDF, \\(h\\), we write\n\\[\nh(X) = c\\left(F_1(X_1), \\dots, F_D(X_D)\\right) \\prod_{i=1}^D f_i(x_i),\n\\]\nwhere \\(c\\) is the density of the copula. Most often, we’d model the log of the PDF\n\\[\n\\log h(X) = \\log c\\left(F_1(X_1), \\dots, F_D(X_D)\\right) + \\sum_{i=1}^D \\log f_i(X_i).\n\\]\nNotice that \\(\\sum_{i=1}^D \\log f_i(X_i)\\) is just the usual sum over marginal log-densities. Let’s rewrite the other term a little bit and explicitly write the parameters we’re conditioning on\n\\[\n\\begin{aligned}\n\\log h(X) &= \\log c\\left(u_1, \\dots, u_D \\vert \\theta_{c}\\right) + \\sum_{i=1}^D \\log f_i(X_i \\vert \\theta_i) \\\\\nu_i &= F_i(X_i \\vert \\theta_i)\n\\end{aligned}\n\\]\nThe main difference when modeling with a copula is\n\nWe need to use the CDFs \\(F_i(X_i \\vert \\theta_i)\\) as well as the pdfs.\nWe need to code up some function \\(\\log c\\left(u_1, \\dots, u_D\\vert \\theta_c\\right)\\) that takes as input the data \\(X\\) after it’s been transformed to \\([0,1]^D\\) by our CDFs and outputs a density."
  },
  {
    "objectID": "phd/articles/methods/data/ukcp/index.html",
    "href": "phd/articles/methods/data/ukcp/index.html",
    "title": "UKCP Data",
    "section": "",
    "text": "Code\nlibrary(bggjphd)\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(cowplot)\nlibrary(glue)\nlibrary(leaflet)\ntheme_set(theme_bggj())"
  },
  {
    "objectID": "phd/articles/methods/data/ukcp/index.html#maximum-precipitation",
    "href": "phd/articles/methods/data/ukcp/index.html#maximum-precipitation",
    "title": "UKCP Data",
    "section": "Maximum precipitation",
    "text": "Maximum precipitation\n\n\nCode\np &lt;- full_data |&gt; \n  ggplot(aes(max_precip, y = after_stat(density))) +\n  geom_histogram(bins = 100) +\n  scale_x_continuous(\n    limits = c(0, NA),\n    expand = expansion()\n  ) +\n  scale_y_continuous(\n    expand = expansion()\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of station-wise maximum precipitation over the period\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/figure1.png\",\n  width = 8, height = 0.621 * 8, scale = 1.3\n)\n\n\n\n\n\nCode\nfull_data |&gt; \n  stations_to_sf() |&gt; \n  points_to_grid() |&gt; \n  spatial_plot(max_precip)\n\n\n\n\n\n\n\n\nCode\np &lt;- full_data |&gt; \n  ggplot(aes(proj_x, proj_y, fill = max_precip)) +\n  geom_raster(\n    interpolate = TRUE\n  ) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(full_data$proj_x), pretty(full_data$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(full_data$proj_y), pretty(full_data$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  theme(\n    plot.margin = margin(t = 5, r = 25, b = 5, l = 5)\n  ) +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = \"Maximum Precipitation\",\n    title = \"Spatial distribution of maximum precipitation\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/figure2.png\",\n  width = 8, height = 0.621 * 8, scale = 1.3\n)"
  },
  {
    "objectID": "phd/articles/methods/data/ukcp/index.html#minimum-precipitation",
    "href": "phd/articles/methods/data/ukcp/index.html#minimum-precipitation",
    "title": "UKCP Data",
    "section": "Minimum precipitation",
    "text": "Minimum precipitation\n\n\nCode\np &lt;- full_data |&gt; \n  ggplot(aes(min_precip, y = after_stat(density))) +\n  geom_histogram(bins = 100) +\n  scale_x_continuous(\n    limits = c(0, NA),\n    expand = expansion()\n  ) +\n  scale_y_continuous(\n    expand = expansion()\n  ) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Distribution of station-wise minimum precipitation over the period\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/figure3.png\",\n  width = 8, height = 0.621 * 8, scale = 1.3\n)\n\n\n\n\n\nCode\np &lt;- full_data |&gt; \n  ggplot(aes(proj_x, proj_y, fill = min_precip)) +\n  geom_raster(\n    interpolate = TRUE\n  ) +\n  scale_x_continuous(\n    expand = expansion(),\n    breaks = c(range(full_data$proj_x), pretty(full_data$proj_x))\n  ) +\n  scale_y_continuous(\n    expand = expansion(),\n    breaks = c(range(full_data$proj_y), pretty(full_data$proj_y))\n  ) +\n  scale_fill_viridis_c() +\n  theme(\n    # legend.position = \"top\",\n    plot.margin = margin(t = 5, r = 25, b = 5, l = 5)\n  ) +\n  labs(\n    x = \"X Projection\",\n    y = \"Y Projection\",\n    fill = \"Minimum Precipitation\",\n    title = \"Spatial distribution of minimum precipitation\"\n  )\nggsave(\n  plot = p,\n  filename = \"Figures/figure4.png\",\n  width = 8, height = 0.621 * 8, scale = 1.3\n)"
  },
  {
    "objectID": "phd/articles/theory/precision/index.html",
    "href": "phd/articles/theory/precision/index.html",
    "title": "Constructing Precision Matrices based on Correlated Gaussian Samples",
    "section": "",
    "text": "Introduction\nHere is a draft of a method to construct precision matrices based on correlated Gaussian samples with mean zero and variance one.\n\n\nMethod\nLet \\(y_t\\) be a random vector of size \\(J\\) and \\(t \\in \\left\\{1, \\dots, T\\right\\}\\), where \\(T\\) is the number of temporal replicates. We assume that \\(y_t \\sim \\mathcal N(0, Q^{-1})\\) and that the marginal variance of the \\(i\\)-th element of \\(y_t\\), \\(y_{i,t}\\), is \\(1\\). This means that the diagonal of \\(Q^{-1}\\) is a vector of ones, and that\n\\[\nE(y_t) = 0, \\qquad \\mathrm{cov}(y_t) = Q^{-1}.\n\\]\nFurthermore, it is assumed that Q is a sparse precision matrix. Using the properties of Gaussian conditional distributions, we have\n\\[\nE(y_{i,t}|y_{-i,t})=-Q_{i,i}^{-1} \\sum_{j\\in\\mathcal A_i, j\\neq i} Q_{i,j}y_{j,t},\n\\]\n\\[\n\\mathrm{Prec}(y_{i,t}|y_{-i, t})   = Q_{i,i} = (\\mathrm{var(y_{i,t}|y_{-i, t})})^{-1}=\\tau_i^{-2},\n\\]\nwhere \\(\\mathcal A_i\\) is the set containing the neighbors of site \\(i\\), i.e. the sites that are such that \\(Q_{i,j} \\neq 0\\) if \\(j \\in \\mathcal A_i\\).\nAssume that we have realizations of \\(y_1, \\dots y_t\\) that can be used to infer the precision matrix \\(Q\\). We set up a regression model to estimate the non-zero elements of \\(Q\\). Here, we consider \\(y_{i,t}\\) as a realization, i.e. as an observation. The regression model for each site, \\(i\\), will be\n\\[\ny_{i,t} = \\sum_{j\\in\\mathcal A, j\\neq i} \\beta_{i,j}y_{j,t} + \\varepsilon_{i, t}, \\quad t\\in \\left\\{1, \\dots, T\\right\\}.\n\\]\nAt each site \\(i\\), we estimate the parameter vector \\(\\beta_i\\) with\n\\[\n\\hat\\beta_i = (X_i^TX_i)^{-1}X_i^Ty_i,\n\\]\nwhere\n\\[\nX_i = \\begin{pmatrix}\ny_{j_{1, i}, 1} & \\dots & y_{j_{m, i}, 1} \\\\\n\\vdots & \\vdots & \\vdots \\\\\ny_{j_{1, i}, T} & \\dots & y_{j_{m, i}, T}\n\\end{pmatrix},\n\\]\nand \\(y_{j_{l, i}, 1}\\) is the \\(l\\)-th neighbor oy \\(y_{i, t}\\) at time \\(t\\). The variance of \\(\\varepsilon_{i, t}\\) is \\(\\tau_i^2\\) and it is estimated with\n\\[\n\\hat\\tau_i^2 = T^{-1}(y_i - X_i\\hat\\beta_i)^T(y_i - X_i\\hat\\beta_i).\n\\]\nThe next step is to transform \\(\\hat\\beta_i\\) and \\(\\hat\\tau_i^2\\) such that they give estimates of the elements of \\(Q\\), namely\n\\[\n\\hat Q_{i, j} = \\begin{cases}\n-\\hat\\tau_i^2\\hat\\beta_{i, j}, \\quad \\text{if } i \\neq j, \\\\\n\\hat\\tau_i^2, \\qquad \\quad \\text{ if } i = j,\n\\end{cases}\n\\]\nwhere \\(\\hat\\beta_{i, j}\\) is the \\(j\\)-th element og \\(\\hat\\beta_i\\). Let \\(\\hat B\\) be a matrix with \\((i, j)\\)-th element \\(\\hat\\beta_{i, j}\\). Note that \\(\\hat \\beta_{i, i} = 0\\), and thus \\(\\hat B_{i, i} = 0\\). Furthermore let \\(\\hat K\\) be a diagonal matrix such that\n\\[\n\\hat K = \\mathrm{diag}\\left(\\hat\\tau_1^{-2}, \\dots, \\hat\\tau_J^{-2}\\right).\n\\]\nAn estimate of Q can now be presented as\n\\[\n\\hat Q = \\hat K(I + \\hat B),\n\\]\nwhere \\(I\\) is an identity matrix of size \\(J\\).\nWe have to make sure that \\(\\hat Q\\) is symmetric. This can be achieved by setting\n\\[\n\\tilde Q{i, j} = \\tilde Q_{j, i} = \\frac12(\\hat\\tau_i^{-2}\\hat\\beta_{i, j} + \\hat\\tau_j^{-2}\\hat\\beta_{j, i}),\n\\]\nand defining new regression parameters \\(\\tilde \\beta_{i, j}\\) that are such that\n\\[\n\\hat\\tau_i^{-2}\\tilde\\beta_{i,j} = \\tilde Q_{ij} = \\tilde Q_{j, i} = \\hat \\tau_j^{-2}\\tilde \\beta_{j, i},\n\\]\nwhich gives\n\\[\n\\tilde\\beta_{i, j} = \\hat\\tau_i^{2}\\tilde Q_{i, j}, \\quad \\tilde\\beta_{j, i} = \\hat\\tau_j^{2}\\tilde Q_{i, j},\n\\]\nand let \\(\\tilde Q\\) and \\(\\tilde B\\) be the matrices containing the \\(\\tilde Q_{i,j}\\)’s and the \\(\\tilde \\beta_{i, j}\\)’s.\nWe can not be sure of \\(\\tilde Q\\) being positive definite. One way to check whether the matrix is positive definite or not, is to compute the Cholesky decomposition of \\(\\tilde Q\\), that is, \\(\\tilde Q = LL^T\\), and check whether all the diagonal elements of L are positive. If the matrix \\(\\tilde Q\\) is invertible then it is more likely that it is positive definite, while if \\(\\tilde Q\\) is not invertible then it is not positive definite. The estimated precision matrix, \\(\\tilde Q\\), is invertible if \\((i + \\tilde B)\\) is invertible, where \\(\\tilde Q = \\hat K(I + \\tilde B)\\). Strictly diagonally dominant matrices are invertible. In general, the \\(n \\times n\\) A, with elements \\(\\left\\{a_{i, j}\\right\\}_{i, j}\\), is strictly diagonally dominant if\n\\[\n\\vert a_{i, i}\\vert &gt; \\sum_{j\\neq i} \\vert a_{i, j}\\vert, \\qquad 1\\leq i \\leq n.\n\\]\nThe matrix \\((I + \\tilde B)\\) is strictly diagonally dominant if\n\\[\n1 &gt; \\sum_{j \\in \\mathcal A_i} \\vert \\tilde \\beta_{i, j}|, \\qquad 1 \\leq i \\leq J,\n\\]\nfor all \\(i \\in \\left\\{1, \\dots, J \\right\\}\\). Alternatively, \\(\\lambda_i \\in (0, 1)\\) is found for each \\(i\\) to tune \\(\\tilde Q\\) such taht it is strictly diagonally dominant, using\n\\[\n\\hat\\tau_i^{-2} &gt; \\lambda_i \\sum_{j\\in\\mathcal A_i, j\\neq i} \\vert \\tilde Q_{i, j} \\vert, \\qquad 1\\leq i \\leq J,\n\\]"
  },
  {
    "objectID": "phd/articles/results/copula/index.html",
    "href": "phd/articles/results/copula/index.html",
    "title": "T-Copula",
    "section": "",
    "text": "Code\nlibrary(bggjphd)\nlibrary(tidyverse)\nlibrary(progressr)\nlibrary(future)\nlibrary(bayesplot)\nlibrary(GGally)\nlibrary(scales)\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(arrow)\nlibrary(tictoc)\nlibrary(broom)\nlibrary(corrr)\nlibrary(patchwork)\ntheme_set(theme_half_open())"
  },
  {
    "objectID": "phd/articles/results/copula/index.html#mean-values",
    "href": "phd/articles/results/copula/index.html#mean-values",
    "title": "T-Copula",
    "section": "Mean values",
    "text": "Mean values\n\n\nCode\nmultivariate &lt;- read_parquet(\"Data/multivariate.parquet\")\n\nplot_dat &lt;- multivariate |&gt; \n  select(station, model_type, type = term, estimate) |&gt; \n  summarise(\n    mean = mean(estimate),\n    sd = sd(estimate),\n    .by = c(type, model_type)\n  ) |&gt; \n  inner_join(\n    neighbor_types,\n    by = \"type\"\n  ) \n\nmax_est &lt;- max(plot_dat$mean, na.rm = T)\nmin_est &lt;- min(plot_dat$mean, na.rm = T)\nscale_size &lt;- max(abs(max_est), abs(min_est), na.rm = T)\nlimits &lt;- c(-1, 1) * scale_size\n\n\np &lt;- plot_dat |&gt; \n  ggplot(aes(diff_x, diff_y, fill = mean)) +\n  geom_raster() + \n  scale_fill_distiller(type = \"div\", palette = \"RdBu\", limits = limits, direction = 1) +\n  facet_wrap(\"model_type\") +\n  labs(\n    x = NULL,\n    y = NULL\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/mean_neighbor_effect_multivariate.png\",\n  width = 8, height = 0.621 * 8, scale = 1.2,\n  bg = \"white\"\n)\n\n\n\n\n\nCode\np &lt;- plot_dat |&gt; \n  select(-sd) |&gt; \n  pivot_wider(names_from = model_type, values_from = mean) |&gt; \n  ggplot(aes(mcmc, ml)) +\n  geom_abline(intercept = 0, slope = 1, lty = 2) +\n  geom_point() +\n  labs(\n    x = \"Neighour effect from spatial model\",\n    y = \"Neighbour effect from ML model\",\n    title = \"Comparison of Mean of neighbour effects in ML and MCMC models\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/compare_mean_neighbor_effect_multivariate.png\",\n  width = 8, height = 0.621 * 8, scale = 1.2,\n  bg = \"white\"\n)"
  },
  {
    "objectID": "phd/articles/results/copula/index.html#spatial-distribution",
    "href": "phd/articles/results/copula/index.html#spatial-distribution",
    "title": "T-Copula",
    "section": "Spatial Distribution",
    "text": "Spatial Distribution\n\nMaximum Likelihood\n\n\nCode\nplot_dat &lt;- multivariate |&gt; \n  filter(model_type == \"ml\") |&gt; \n  select(station, type = term, estimate = statistic) |&gt; \n  mutate(\n    estimate = case_when(\n      estimate &gt; quantile(estimate, 0.995) ~ quantile(estimate, 0.995),\n      estimate &lt; quantile(estimate, 0.005) ~ quantile(estimate, 0.005),\n      TRUE ~ estimate\n    ),\n    .by = type\n  ) |&gt; \n  inner_join(\n    stations,\n    by = \"station\"\n  )\n\n\nmax_est &lt;- max(plot_dat$estimate, na.rm = T)\nmin_est &lt;- min(plot_dat$estimate, na.rm = T)\nscale_size &lt;- max(abs(max_est), abs(min_est), na.rm = T)\nlimits &lt;- c(-1, 1) * scale_size\n\n# plot_dat |&gt;\n#   filter(type == \"ww\") |&gt;\n#   ggplot(aes(proj_x, proj_y, fill = estimate)) +\n#   geom_raster(interpolate = TRUE) +\n#   scale_fill_distiller(type = \"div\", palette = \"RdBu\", limits = limits) +\n#   facet_wrap(\"type\")\n\nplots &lt;- plot_dat |&gt; \n  mutate(term = type) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(proj_x, proj_y, fill = estimate)) +\n                    geom_raster(interpolate = TRUE) +\n                    scale_fill_distiller(\n                      type = \"div\",\n                      palette = \"RdBu\",\n                      limits = limits,\n                      direction = 1\n                    ) +\n                    facet_wrap(\"term\") +\n                    theme_void() +\n                    labs(\n                      fill = \"t-statistic\"\n                    )\n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Spatial distributions of neighbor effects in t-copula (ML Predictions)\",\n    subtitle = \"Shown as t-statistics of linear model coefficients\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/spatial_dist_by_neighbor_type_ml.png\",\n  width = 8, height = 8, scale = 1,\n  bg = \"white\",\n  dpi = 320\n)\n\n\n\n\n\nSpatial Model\n\n\nCode\nplot_dat &lt;- multivariate |&gt; \n  filter(model_type == \"mcmc\") |&gt; \n  select(station, type = term, estimate = statistic) |&gt; \n  mutate(\n    estimate = case_when(\n      estimate &gt; quantile(estimate, 0.995) ~ quantile(estimate, 0.995),\n      estimate &lt; quantile(estimate, 0.005) ~ quantile(estimate, 0.005),\n      TRUE ~ estimate\n    ),\n    .by = type\n  ) |&gt; \n  inner_join(\n    stations,\n    by = \"station\"\n  )\n\n\nmax_est &lt;- max(plot_dat$estimate, na.rm = T)\nmin_est &lt;- min(plot_dat$estimate, na.rm = T)\nscale_size &lt;- max(abs(max_est), abs(min_est), na.rm = T)\nlimits &lt;- c(-1, 1) * scale_size\n\n# plot_dat |&gt;\n#   filter(type == \"ww\") |&gt;\n#   ggplot(aes(proj_x, proj_y, fill = estimate)) +\n#   geom_raster(interpolate = TRUE) +\n#   scale_fill_distiller(type = \"div\", palette = \"RdBu\", limits = limits) +\n#   facet_wrap(\"type\")\n\nplots &lt;- plot_dat |&gt; \n  mutate(term = type) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(proj_x, proj_y, fill = estimate)) +\n                    geom_raster(interpolate = TRUE) +\n                    scale_fill_distiller(\n                      type = \"div\",\n                      palette = \"RdBu\",\n                      limits = limits,\n                      direction = 1\n                    ) +\n                    facet_wrap(\"term\") +\n                    theme_void() +\n                    labs(\n                      fill = \"t-statistic\"\n                    )\n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Spatial distributions of neighbor effects in t-copula (MCMC Predictions)\",\n    subtitle = \"Shown as t-statistics of linear model coefficients\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/spatial_dist_by_neighbor_type_mcmc.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)"
  },
  {
    "objectID": "phd/articles/results/copula/index.html#parameter-correlations",
    "href": "phd/articles/results/copula/index.html#parameter-correlations",
    "title": "T-Copula",
    "section": "Parameter Correlations",
    "text": "Parameter Correlations\n\nMaximum Likelihood\n\n\nCode\nplot_dat &lt;- multivariate |&gt; \n  filter(model_type == \"ml\") |&gt; \n  select(station, type = term, estimate) |&gt; \n  pivot_wider(names_from = type, values_from = estimate) |&gt; \n  ungroup() |&gt; \n  select(-station) |&gt; \n  correlate(method = \"pearson\", use = \"pairwise.complete.obs\", quiet = T) |&gt; \n  pivot_longer(c(-term), names_to = \"term2\", values_to = \"correlation\") |&gt; \n  inner_join(\n    neighbor_types,\n    by = c(\"term2\" = \"type\")\n  )\n\nmax_cor &lt;- max(plot_dat$correlation, na.rm = T)\nmin_cor &lt;- min(plot_dat$correlation, na.rm = T)\nscale_size &lt;- max(abs(max_cor), abs(min_cor), na.rm = T)\nlimits &lt;- c(-1, 1) * scale_size\n\n\n\n\nplots &lt;- plot_dat |&gt; \n  mutate(type = term) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(diff_x, diff_y, fill = correlation)) +\n                    geom_raster() +\n                    # scale_fill_viridis_c(guide = guide_colorbar(), limits = limits) +\n                    scale_fill_distiller(\n                      type = \"div\", \n                      palette = \"RdBu\", \n                      limits = limits,\n                      direction = 1\n                    ) +\n                    facet_wrap(\"term\") +\n                    theme_void() \n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Correlations between effects of different neighbors (ML predictions)\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/neighbor_type_correlations_ml.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)\n\n\n\n\n\nSpatial model\n\n\nCode\nplot_dat &lt;- multivariate |&gt; \n  filter(model_type == \"mcmc\") |&gt; \n  select(station, type = term, estimate) |&gt; \n  pivot_wider(names_from = type, values_from = estimate) |&gt; \n  ungroup() |&gt; \n  select(-station) |&gt; \n  correlate(method = \"pearson\", use = \"pairwise.complete.obs\", quiet = T) |&gt; \n  pivot_longer(c(-term), names_to = \"term2\", values_to = \"correlation\") |&gt; \n  inner_join(\n    neighbor_types,\n    by = c(\"term2\" = \"type\")\n  )\n\nmax_cor &lt;- max(plot_dat$correlation, na.rm = T)\nmin_cor &lt;- min(plot_dat$correlation, na.rm = T)\nscale_size &lt;- max(abs(max_cor), abs(min_cor), na.rm = T)\nlimits &lt;- c(-1, 1) * scale_size\n\n\n\n\nplots &lt;- plot_dat |&gt; \n  mutate(type = term) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(diff_x, diff_y, fill = correlation)) +\n                    geom_raster() +\n                    # scale_fill_viridis_c(guide = guide_colorbar(), limits = limits) +\n                    scale_fill_distiller(\n                      type = \"div\", \n                      palette = \"RdBu\", \n                      limits = limits,\n                      direction = 1\n                    ) +\n                    facet_wrap(\"term\") +\n                    theme_void() \n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Correlations between effects of different neighbors (MCMC predictions)\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/neighbor_type_correlations_mcmc.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)"
  },
  {
    "objectID": "phd/articles/results/copula/index.html#mean-values-1",
    "href": "phd/articles/results/copula/index.html#mean-values-1",
    "title": "T-Copula",
    "section": "Mean values",
    "text": "Mean values\n\n\nCode\nunivariate &lt;- read_parquet(\"Data/univariate.parquet\")\n\nplot_dat &lt;- univariate |&gt; \n  select(station, model_type, type, estimate) |&gt; \n  summarise(\n    mean = mean(estimate),\n    sd = sd(estimate),\n    .by = c(type, model_type)\n  ) |&gt; \n  inner_join(\n    neighbor_types,\n    by = \"type\"\n  ) \n\nmax_est &lt;- max(plot_dat$mean, na.rm = T)\nmin_est &lt;- min(plot_dat$mean, na.rm = T)\nscale_size &lt;- max(abs(max_est), abs(min_est), na.rm = T)\nlimits &lt;- c(0, 1) * scale_size\n\n\np &lt;- plot_dat |&gt; \n  ggplot(aes(diff_x, diff_y, fill = mean)) +\n  geom_raster() + \n  scale_fill_distiller(type = \"div\", palette = \"RdBu\", limits = limits, direction = 1) +\n  facet_wrap(\"model_type\")\n\nggsave(\n  plot = p,\n  filename = \"Figures/mean_neighbor_effect_univariate.png\",\n  width = 8, height = 0.621 * 8, scale = 1.2,\n  bg = \"white\"\n)\n\n\n\n\n\nCode\np &lt;- plot_dat |&gt; \n  select(-sd) |&gt; \n  pivot_wider(names_from = model_type, values_from = mean) |&gt; \n  ggplot(aes(mcmc, ml)) +\n  geom_abline(intercept = 0, slope = 1, lty = 2) +\n  geom_point() +\n  labs(\n    x = \"Neighour effect from spatial model\",\n    y = \"Neighbour effect from ML model\",\n    title = \"Comparison of neighbour effects in ML and MCMC models\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/compare_mean_neighbor_effect_univariate.png\",\n  width = 8, height = 0.621 * 8, scale = 1.2,\n  bg = \"white\"\n)"
  },
  {
    "objectID": "phd/articles/results/copula/index.html#spatial-distribution-1",
    "href": "phd/articles/results/copula/index.html#spatial-distribution-1",
    "title": "T-Copula",
    "section": "Spatial Distribution",
    "text": "Spatial Distribution\n\nMaximum Likelihood\n\n\nCode\nplot_dat &lt;- univariate |&gt; \n  filter(model_type == \"ml\") |&gt; \n  select(station, type, estimate = statistic) |&gt; \n  mutate(\n    estimate = case_when(\n      estimate &gt; quantile(estimate, 0.995) ~ quantile(estimate, 0.995),\n      estimate &lt; quantile(estimate, 0.005) ~ quantile(estimate, 0.005),\n      TRUE ~ estimate\n    ),\n    .by = type\n  ) |&gt; \n  inner_join(\n    stations,\n    by = \"station\"\n  )\n\n\nmax_est &lt;- max(plot_dat$estimate, na.rm = T)\nmin_est &lt;- min(plot_dat$estimate, na.rm = T)\nscale_size &lt;- max(abs(max_est), abs(min_est), na.rm = T)\nlimits &lt;- c(0, 1) * scale_size\n\n# plot_dat |&gt;\n#   filter(type == \"ww\") |&gt;\n#   ggplot(aes(proj_x, proj_y, fill = estimate)) +\n#   geom_raster(interpolate = TRUE) +\n#   scale_fill_distiller(type = \"div\", palette = \"RdBu\", limits = limits) +\n#   facet_wrap(\"type\")\n\nplots &lt;- plot_dat |&gt; \n  mutate(term = type) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(proj_x, proj_y, fill = estimate)) +\n                    geom_raster(interpolate = TRUE) +\n                    # scale_fill_distiller(\n                    #   type = \"div\",\n                    #   palette = \"RdBu\",\n                    #   limits = limits,\n                    #   direction = 1\n                    # ) +\n                    scale_fill_viridis_c(limits = limits) +\n                    facet_wrap(\"term\") +\n                    theme_void() +\n                    labs(\n                      fill = \"t-statistic\"\n                    )\n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Spatial distributions of neighbor effects in t-copula (ML Predictions)\",\n    subtitle = \"Shown as t-statistics of linear model coefficients\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/spatial_dist_by_neighbor_type_ml_univariate.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)\n\n\n\n\n\nSpatial Model\n\n\nCode\nplot_dat &lt;- univariate |&gt; \n  filter(model_type == \"mcmc\") |&gt; \n  select(station, type, estimate) |&gt; \n    mutate(\n    estimate = case_when(\n      estimate &gt; quantile(estimate, 0.995) ~ quantile(estimate, 0.995),\n      estimate &lt; quantile(estimate, 0.005) ~ quantile(estimate, 0.005),\n      TRUE ~ estimate\n    ),\n    .by = type\n  ) |&gt; \n  inner_join(\n    stations,\n    by = \"station\"\n  )\n\n\nmax_est &lt;- max(plot_dat$estimate, na.rm = T)\nmin_est &lt;- min(plot_dat$estimate, na.rm = T)\nscale_size &lt;- max(abs(max_est), abs(min_est), na.rm = T)\nlimits &lt;- c(0, 1) * scale_size\n\n# plot_dat |&gt;\n#   filter(type == \"ww\") |&gt;\n#   ggplot(aes(proj_x, proj_y, fill = estimate)) +\n#   geom_raster(interpolate = TRUE) +\n#   scale_fill_distiller(type = \"div\", palette = \"RdBu\", limits = limits) +\n#   facet_wrap(\"type\")\n\nplots &lt;- plot_dat |&gt; \n  mutate(term = type) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(proj_x, proj_y, fill = estimate)) +\n                    geom_raster(interpolate = TRUE) +\n                    # scale_fill_distiller(\n                    #   type = \"div\",\n                    #   palette = \"RdBu\",\n                    #   limits = limits,\n                    #   direction = 1\n                    # ) +\n                    scale_fill_viridis_c(limits = limits) +\n                    facet_wrap(\"term\") +\n                    theme_void()\n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Spatial distributions of neighbor effects in t-copula (MCMC Predictions)\",\n    subtitle = \"Shown as t-statistics of linear model coefficients\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/spatial_dist_by_neighbor_type_mcmc_univariate.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)"
  },
  {
    "objectID": "phd/articles/results/copula/index.html#parameter-correlations-1",
    "href": "phd/articles/results/copula/index.html#parameter-correlations-1",
    "title": "T-Copula",
    "section": "Parameter Correlations",
    "text": "Parameter Correlations\n\nMaximum Likelihood\n\n\nCode\nplot_dat &lt;- univariate |&gt; \n  filter(model_type == \"ml\") |&gt; \n  select(station, type, estimate) |&gt; \n  pivot_wider(names_from = type, values_from = estimate) |&gt; \n  ungroup() |&gt; \n  select(-station) |&gt; \n  correlate(method = \"pearson\", use = \"pairwise.complete.obs\", quiet = T) |&gt; \n  pivot_longer(c(-term), names_to = \"term2\", values_to = \"correlation\") |&gt; \n  inner_join(\n    neighbor_types,\n    by = c(\"term2\" = \"type\")\n  )\n\nmax_cor &lt;- max(plot_dat$correlation, na.rm = T)\nmin_cor &lt;- min(plot_dat$correlation, na.rm = T)\nscale_size &lt;- max(abs(max_cor), abs(min_cor), na.rm = T)\nlimits &lt;- c(0, 1) * scale_size\n\n\n\n\nplots &lt;- plot_dat |&gt; \n  mutate(type = term) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(diff_x, diff_y, fill = correlation)) +\n                    geom_raster() +\n                    # scale_fill_viridis_c(guide = guide_colorbar(), limits = limits) +\n                    scale_fill_distiller(\n                      type = \"div\",\n                      palette = \"RdBu\",\n                      limits = limits,\n                      direction = 1\n                    ) +\n                    # scale_fill_viridis_c(limits = limits) +\n                    facet_wrap(\"term\") +\n                    theme_void() \n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Correlations between effects of different neighbors (ML predictions)\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/neighbor_type_correlations_ml_univariate.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)\n\n\n\n\n\nSpatial model\n\n\nCode\nplot_dat &lt;- univariate |&gt; \n  filter(model_type == \"mcmc\") |&gt; \n  select(station, type, estimate) |&gt; \n  pivot_wider(names_from = type, values_from = estimate) |&gt; \n  ungroup() |&gt; \n  select(-station) |&gt; \n  correlate(method = \"pearson\", use = \"pairwise.complete.obs\", quiet = T) |&gt; \n  pivot_longer(c(-term), names_to = \"term2\", values_to = \"correlation\") |&gt; \n  inner_join(\n    neighbor_types,\n    by = c(\"term2\" = \"type\")\n  )\n\nmax_cor &lt;- max(plot_dat$correlation, na.rm = T)\nmin_cor &lt;- min(plot_dat$correlation, na.rm = T)\nscale_size &lt;- max(abs(max_cor), abs(min_cor), na.rm = T)\nlimits &lt;- c(0, 1) * scale_size\n\n\n\n\nplots &lt;- plot_dat |&gt; \n  mutate(type = term) |&gt; \n  group_by(type) |&gt; \n  group_nest() |&gt; \n  mutate(\n    plots = map(data, \n                function(data, ...) {\n                  data |&gt; \n                    ggplot(aes(diff_x, diff_y, fill = correlation)) +\n                    geom_raster() +\n                    # scale_fill_viridis_c(guide = guide_colorbar(), limits = limits) +\n                    scale_fill_distiller(\n                      type = \"div\",\n                      palette = \"RdBu\",\n                      limits = limits,\n                      direction = 1\n                    ) +\n                    # scale_fill_viridis_c(limits = limits) +\n                    facet_wrap(\"term\") +\n                    theme_void() \n                }\n    )\n  ) |&gt; \n  select(type, plots) |&gt; \n  pivot_wider(names_from = type, values_from = plots)\n\n\nlayout &lt;- \"\n##A##\n#BCD#\nEF#GH\n#IJK#\n##L##\n\"\n\n\np &lt;- plots$nn[[1]] + \n  plots$nw[[1]] + plots$n[[1]] + plots$ne[[1]] +\n  plots$ww[[1]] + plots$w[[1]] + plots$e[[1]] + plots$ee[[1]] +\n  plots$sw[[1]] + plots$s[[1]] + plots$se[[1]] +\n  plots$ss[[1]] +\n  plot_layout(\n    design = layout, \n    guides = \"collect\"\n  ) +\n  plot_annotation(\n    title = \"Correlations between effects of different neighbors (MCMC predictions)\"\n  )\n\nggsave(\n  plot = p,\n  filename = \"Figures/neighbor_type_correlations_mcmc_univariate.png\",\n  width = 8, height = 8, scale = 1,\n  dpi = 320,\n  bg = \"white\"\n)"
  },
  {
    "objectID": "phd/index.html",
    "href": "phd/index.html",
    "title": "About my PhD",
    "section": "",
    "text": "I’m developing and maintaining this part of my page to go along with my PhD studies for reproducibility and to held me keep my sanity intact for the next years.\nThe main gist of my PhD research will be adding to the Max-and-Smooth method by Hrafnkelsson et al. (2020).\n\nHrafnkelsson, Birgir, Stefan Siegert, Raphaël Huser, Haakon Bakka, and Árni V. Jóhannesson. 2020. “Max-and-Smooth: A Two-Step Approach for Approximate Bayesian Inference in Latent Gaussian Models.” arXiv. https://doi.org/10.48550/arXiv.1907.11969."
  },
  {
    "objectID": "phd/index.html#r-package-on-github",
    "href": "phd/index.html#r-package-on-github",
    "title": "About my PhD",
    "section": "R package on Github",
    "text": "R package on Github\nI’ve put the code and data for any modeling into an R package, bggjphd, which I keep up to date on Github"
  },
  {
    "objectID": "phd/index.html#docker-container",
    "href": "phd/index.html#docker-container",
    "title": "About my PhD",
    "section": "Docker Container",
    "text": "Docker Container\nFor reproducibility I keep a Docker container on Docker Hub that sets up an R environment and all the packages needed to run code from the bggjphd package."
  },
  {
    "objectID": "phd/index.html#data",
    "href": "phd/index.html#data",
    "title": "About my PhD",
    "section": "Data",
    "text": "Data\n\nUKCP\nUKCP Data\nGather data on hourly precipitation"
  },
  {
    "objectID": "phd/index.html#models",
    "href": "phd/index.html#models",
    "title": "About my PhD",
    "section": "Models",
    "text": "Models\n\nMaximum Likelihood Estimation\n\n\nSpatial Smoothing of MLEs"
  },
  {
    "objectID": "talks/hi-vefur/index.html",
    "href": "talks/hi-vefur/index.html",
    "title": "Interview on the University website",
    "section": "",
    "text": "Find the iterview here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bggj",
    "section": "",
    "text": "It was the best of tails, it was the worst of tails: The T-Copula\n\n\nCopulas in Stan: Episode III\n\n\n\nstan\n\n\ncopulas\n\n\ncopulas in stan\n\n\n\nThe Gaussian copula’s inability to model tail dependence can be a serious limitation in practice. This post introduces the t-copula, which shares many convenient properties with the Gaussian copula while also capturing the tendency of extreme events to occur together. We’ll examine its mathematical properties, discuss when to use it instead of the Gaussian copula, and provide a complete implementation in Stan.\n\n\n\n\n\nDec 12, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle Introduction: The Gaussian Copula\n\n\nCopulas in Stan: Episode II\n\n\n\nstan\n\n\ncopulas\n\n\ncopulas in stan\n\n\n\nThis post continues the series on copulas in Stan by introducing the Gaussian copula, discussing its properties, applications, and providing examples of how to implement it in Stan.\n\n\n\n\n\nSep 23, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nIf It Bleeds, We Can Kill It\n\n\nCopulas in Stan: Episode 1\n\n\n\nstan\n\n\ncopulas\n\n\ncopulas in stan\n\n\n\nThis is the first post in what’s going to be a series on using Copulas in Stan. Each post is going to be short to keep me from postponing writing them. In this post I lightly introduce the series and give a quick primer on copulas.\n\n\n\n\n\nSep 15, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a Gaussian AR(1) Copula to Generalized Extreme Value Margins\n\n\nA simulation study of the differences between AR(1) copula and i.i.d copula\n\n\n\nenglish\n\n\nR\n\n\nphd\n\n\nstan\n\n\nbayesian\n\n\nspatial statistics\n\n\ncopulas\n\n\n\nAnother step in my PhD studies is to apply multivariate copulas to data with Generalized Extreme Value marginal distributions. This is important to enable us to model dependence on the data leve, as opposed to the latent (parameter) level. \n\n\n\n\n\nFeb 6, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nSpatially Dependent Generalized Extreme Value Parameters\n\n\nUsing Stan to fit a Generalized Extreme Value Distribution with spatially dependent parameters\n\n\n\nenglish\n\n\nR\n\n\nphd\n\n\nstan\n\n\nbayesian\n\n\nspatial statistics\n\n\nhierarchical modeling\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nMaking interactive animations with ggplotly\n\n\nSimple examples of how to use the frame aesthetic in ggplotly() for easy animations\n\n\n\nenglish\n\n\nR\n\n\nggplot\n\n\nplotly\n\n\n\nI’ve recently been making a lot of plotly graphs for my Icelandic website metill.is, where I look at official data on current topics. Since it is so simple to make animations with ggplotly I thought I might write a short tutorial on it. \n\n\n\n\n\nFeb 6, 2023\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\n\n\n\n\n\n\nFetching FTP data from the Ceda Archives\n\n\nUsing R and FTP to programmatically fetch a large amount of data for processing\n\n\n\nenglish\n\n\nR\n\n\nphd\n\n\nbig data\n\n\nscraping\n\n\n\nFor my PhD modeling I needed to fetch a large amount of data from the CEDA Archive, specifically I use hourly precipitation projections from UKCP Local Projections on a 5km grid over the UK for 1980-2080. The hourly precipitation projections are stored in 720 files that are all approximately 120mb to 130mb. Here I write out my processing in case someone needs help with doing something similar. \n\n\n\n\n\nFeb 3, 2023\n\n\nBrynjólfur Gauti Guðrúnar Jónsson\n\n\n\n\n\n\nNo matching items"
  }
]